{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05d518ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linghao/env/p37_r/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This implementation is based on the guidance from the SpinningUp article:\n",
    "https://spinningup.openai.com/en/latest/algorithms/td3.html\n",
    "\"\"\"\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "## ----------------- Helpers ---------------- ## \n",
    "\n",
    "def flatten(nestedTuple: tuple):\n",
    "    \"\"\"\n",
    "    flatten input dimensions for NNs\n",
    "    \"\"\"\n",
    "    merged = []\n",
    "    for item in nestedTuple:\n",
    "        if isinstance(item, tuple) or isinstance(item, list):\n",
    "            for i in item:\n",
    "                merged.append(i)\n",
    "        else:\n",
    "            merged.append(item)  \n",
    "    return merged   \n",
    "\n",
    "## This function aims to help updating target actor/critic parameters\n",
    "def targetUpdater(normalModel, targetModel, lRate = 0.3):\n",
    "    for normalParam, targetParam in zip(normalModel.parameters(), targetModel.parameters()):\n",
    "        targetParam.data.copy_(targetParam.data * (1.0 - lRate) + normalParam.data * lRate)\n",
    "## This function aims to initilize target actor/critic parameters with normal Actor/Critc \n",
    "def initializer(normalModel, targetModel):\n",
    "    for normalParam, targetParam in zip(normalModel.parameters(), targetModel.parameters()):\n",
    "        targetParam.data.copy_(normalParam.data)\n",
    "    \n",
    "## ------------------ Actor ----------------- ## \n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"\n",
    "    hidden layer can be further defined using another tuple e.g. (30, (128,64), 8) \n",
    "    or simply define in one tuple (30, 128, 64, 8)\n",
    "    \"\"\"\n",
    "    def __init__(self, structure, use_dropout = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        ## Activation function:\n",
    "        self.activation  = nn.ReLU()\n",
    "        \n",
    "        # Automatic population of FC network\n",
    "        dim = flatten(structure)\n",
    "        dim_shifted = dim[1:]\n",
    "        \n",
    "        for i in range(len(dim)-1):\n",
    "            layers.append(nn.Linear(dim[i], dim_shifted[i]))\n",
    "            \n",
    "            ## Just in case for mitigating overfitting ...\n",
    "            if use_dropout and i != len(dim)-2:\n",
    "                layers.append(nn.Dropout(p = 0.2))\n",
    "            \n",
    "            if i != len(dim)-2:\n",
    "                layers.append(self.activation)\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, states):\n",
    "        output = self.model(states)\n",
    "        ## This is to scale back the actions to (-1, 1) for Ant environment !Need to change for other environmen\n",
    "        output = torch.tanh(output) \n",
    "        \n",
    "        return output\n",
    "    \n",
    "## -------------------Critic ---------------- ## \n",
    "class Critic(nn.Module):\n",
    "    \"\"\"\n",
    "    Critic network input dimension must equal the states_dim + actions_dim\n",
    "    \"\"\"\n",
    "    def __init__(self, structure, use_dropout = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        ## Activation function:\n",
    "        self.activation  = nn.ReLU()\n",
    "        \n",
    "        # Automatic population of FC network\n",
    "        dim = flatten(structure)\n",
    "        dim_shifted = dim[1:]\n",
    "        \n",
    "        for i in range(len(dim)-1):\n",
    "            layers.append(nn.Linear(dim[i], dim_shifted[i]))\n",
    "            \n",
    "            ## Just in case for mitigating overfitting ...\n",
    "            if use_dropout and i != len(dim)-2:\n",
    "                layers.append(nn.Dropout(p = 0.2))\n",
    "                \n",
    "            if i != len(dim)-2:\n",
    "                layers.append(self.activation)\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, states, actions):\n",
    "        \n",
    "        ## concatenate states and actions, order shouldn't really matter\n",
    "        ## Normalise action for ANYmal\n",
    "        inputs = torch.cat([states, actions], 1)\n",
    "        output = self.model(inputs)\n",
    "        return output\n",
    "\n",
    "\n",
    "## -------------------Replay ---------------- ## \n",
    "class ReplayMemory():\n",
    "    \n",
    "    def __init__(self, size=500):\n",
    "        self.size = size\n",
    "        self.memory = []\n",
    "        \n",
    "    def addMemory(self, transition):\n",
    "        \"\"\"\n",
    "        Transition composition:\n",
    "        State, Action, Reward, `State\n",
    "        \"\"\"\n",
    "        if self.isFull():\n",
    "            ## If memory is full, remove the first stored elements\n",
    "            self.memory.pop(0)\n",
    "            \n",
    "        self.memory.append(transition)\n",
    "                \n",
    "    def sample(self, batchSize):\n",
    "        ## This is a very slow implementation - should be optimized if possible\n",
    "        batch = random.sample(self.memory, batchSize)\n",
    "        state, action, reward, stateNext, terminated = [], [], [], [], []\n",
    "        for transition in batch:\n",
    "            state.append(transition[0])\n",
    "            action.append(transition[1])\n",
    "            reward.append(transition[2])\n",
    "            stateNext.append(transition[3])\n",
    "            terminated.append(transition[4])\n",
    "        \n",
    "        return state, action, reward, stateNext, terminated\n",
    "    \n",
    "    def isFull(self):\n",
    "        return len(self.memory)==self.size\n",
    "    \n",
    "    def getSize(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "## ------------------- Target policy smoothing ---------------- ## \n",
    "class policySmooth:\n",
    "    def __init__(self,  actionSpace, std = 0.1, clip = 0.3):\n",
    "        \n",
    "        self.std = std\n",
    "        self.clip = clip\n",
    "        self.size = actionSpace.shape[0]\n",
    "        ## normalise upper lower limit for ANYmal\n",
    "        self.high = actionSpace.high / 80\n",
    "        self.low = actionSpace.low / 80\n",
    "        \n",
    "    def getAction(self, actionIn, noiseClip = True):\n",
    "        noise = np.random.normal(0, self.std, self.size)\n",
    "        \n",
    "        ## For action no clipping, for target action apply clip\n",
    "        if noiseClip:\n",
    "            noise = np.clip(noise, -self.clip, self.clip)\n",
    "        action = actionIn + noise\n",
    "        \n",
    "        return np.clip(action, self.low, self.high)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1dddd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class TD3Agent:\n",
    "    \n",
    "    def __init__(self, actionSpace, actorStructure:tuple, criticStructure:tuple, use_dropout = False, actorLRate = 0.0003, criticLRate = 0.0003, criticLoss = 'HL', gamma = 0.9, targetUpdaterLRate = 0.3, replayMemorySize = 500, batchSize = 128, policyDelay = 2, gpu = True):\n",
    "        \n",
    "        self.actor = Actor(structure=actorStructure, use_dropout = use_dropout)\n",
    "        self.critic1 = Critic(structure=criticStructure, use_dropout = use_dropout)\n",
    "        self.critic2 = Critic(structure=criticStructure, use_dropout = use_dropout)\n",
    "        \n",
    "        self.targetActor = Actor(structure=actorStructure, use_dropout = use_dropout)\n",
    "        self.targetCritic1 = Critic(structure=criticStructure, use_dropout = use_dropout)\n",
    "        self.targetCritic2 = Critic(structure=criticStructure, use_dropout = use_dropout)\n",
    "        \n",
    "        self.batchSize = batchSize\n",
    "        self.targetUpdaterLRate = targetUpdaterLRate\n",
    "        self.gamma = gamma\n",
    "        self.gpu = gpu\n",
    "        self.policyDelay = policyDelay\n",
    "        if self.gpu:\n",
    "            self.cuda()\n",
    "        \n",
    "        ## initialise target models \n",
    "        initializer(self.actor, self.targetActor)\n",
    "        initializer(self.critic1, self.targetCritic1)\n",
    "        initializer(self.critic2, self.targetCritic2)\n",
    "        \n",
    "        ## initialise replay memory and random process\n",
    "        self.pSmooth = policySmooth(actionSpace)\n",
    "        self.rMemory = ReplayMemory(replayMemorySize)\n",
    " \n",
    "        \n",
    "        ## initialise optimizer\n",
    "        self.actorOptim = optim.Adam(self.actor.parameters(), lr=actorLRate)\n",
    "        self.criticOptim1 = optim.Adam(self.critic1.parameters(), lr=criticLRate)\n",
    "        self.criticOptim2 = optim.Adam(self.critic2.parameters(), lr=criticLRate)\n",
    "        \n",
    "        ## initialise Loss for critic, either Huber (default) or MSE (otherwise)\n",
    "        if criticLoss == 'HL':\n",
    "            self.criticCriteria1 = nn.HuberLoss()\n",
    "            self.criticCriteria2 = nn.HuberLoss()\n",
    "        \n",
    "        else:\n",
    "            self.criticCriteria1 = nn.MSELoss()\n",
    "            self.criticCriteria2 = nn.MSELoss()\n",
    "    def updatePolicy(self, num):\n",
    "        ## Assuming sufficient replay memory has been acquired\n",
    "        \n",
    "        ## 1. Sample (S, A, R, S`) in replay memory\n",
    "\n",
    "        state, action, reward, stateNext, terminated = self.rMemory.sample(self.batchSize)\n",
    "\n",
    "        if self.gpu:\n",
    "            state = torch.cuda.FloatTensor(state)\n",
    "            action = torch.cuda.FloatTensor(action)\n",
    "            reward = torch.cuda.FloatTensor(reward)\n",
    "            stateNext = torch.cuda.FloatTensor(stateNext)\n",
    "            terminated = torch.cuda.FloatTensor(terminated)\n",
    "\n",
    "        else:\n",
    "            state = torch.FloatTensor(state)\n",
    "            action = torch.FloatTensor(action)\n",
    "            reward = torch.FloatTensor(reward)\n",
    "            stateNext = torch.FloatTensor(stateNext)\n",
    "            terminated = torch.FloatTensor(terminated)\n",
    "\n",
    "        ## 2. Gradient of actor and critic  \n",
    "\n",
    "        targetAction = self.pSmooth.getAction(self.targetActor(stateNext).detach().cpu().numpy(), noiseClip=True)\n",
    "        if self.gpu:\n",
    "            targetAction = torch.cuda.FloatTensor(targetAction)\n",
    "        else:\n",
    "            targetAction = torch.FloatTensor(targetAction)\n",
    "        Q1 = self.targetCritic1(stateNext.detach(), targetAction.detach())\n",
    "        Q2 = self.targetCritic2(stateNext.detach(), targetAction.detach())\n",
    "\n",
    "        ## SMALLER Q Proceed (this verion consider actions individually)\n",
    "        if Q1.mean() > Q2.mean():\n",
    "            Q = Q2\n",
    "        else:\n",
    "            Q = Q1\n",
    "\n",
    "        y = self.gamma * (1 - terminated) * Q + reward\n",
    "        y.requires_grad_()\n",
    "\n",
    "\n",
    "        yBar1 = self.critic1(state, action)\n",
    "        yBar2 = self.critic2(state, action)\n",
    "\n",
    "        criticLoss1 = self.criticCriteria1(yBar1, y)\n",
    "        criticLoss2 = self.criticCriteria2(yBar2, y)\n",
    "\n",
    "        ## backward losses\n",
    "        self.criticOptim1.zero_grad()\n",
    "        criticLoss1.backward(retain_graph = True)\n",
    "        self.criticOptim1.step()\n",
    "\n",
    "        self.criticOptim2.zero_grad()\n",
    "        criticLoss2.backward()\n",
    "        self.criticOptim2.step()\n",
    "\n",
    "        ## update actor when delay condition is met\n",
    "        if num % self.policyDelay == 0:\n",
    "            actorLoss = -self.critic1(state, self.actor(state)).mean()\n",
    "            self.actorOptim.zero_grad()\n",
    "            actorLoss.backward()\n",
    "            self.actorOptim.step()\n",
    "        ## 3. Update target actor / critic\n",
    "            targetUpdater(self.critic1, self.targetCritic1, self.targetUpdaterLRate)\n",
    "            targetUpdater(self.critic2, self.targetCritic2, self.targetUpdaterLRate)\n",
    "            targetUpdater(self.actor, self.targetActor, self.targetUpdaterLRate)\n",
    "\n",
    "\n",
    "    \n",
    "    ## Add option to discard noise in eval mode\n",
    "    def getAction(self, states, eval = False):\n",
    "        if self.gpu:\n",
    "            states = torch.cuda.FloatTensor(states)\n",
    "        else:\n",
    "            states = torch.FloatTensor(states)\n",
    "        \n",
    "        action = self.actor(states).detach().cpu().numpy()\n",
    "        \n",
    "        # Action selection without noise clip\n",
    "        if not eval:\n",
    "            action = self.pSmooth.getAction(action, noiseClip=False)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def cuda(self):\n",
    "        ## Move all models to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            self.actor.cuda()\n",
    "            self.critic1.cuda()\n",
    "            self.critic2.cuda()\n",
    "            self.targetActor.cuda()\n",
    "            self.targetCritic1.cuda()\n",
    "            self.targetCritic2.cuda()\n",
    "        else:\n",
    "            raise ValueError(\"No Cuda Available\")\n",
    "            \n",
    "    def cpu(self):\n",
    "        ## Move all models back to CPU\n",
    "        self.actor.cpu()\n",
    "        self.critic1.cpu()\n",
    "        self.critic2.cpu()\n",
    "        self.targetActor.cpu()\n",
    "        self.targetCritic1.cpu()\n",
    "        self.targetCritic2.cpu()\n",
    "        \n",
    "    def evalMode(self):\n",
    "        ## Evaluation mode for Agent showcase \n",
    "        self.actor.eval()\n",
    "        self.critic1.eval()\n",
    "        self.critic2.eval()\n",
    "        self.targetActor.eval()\n",
    "        self.targetCritic1.eval()  \n",
    "        self.targetCritic2.eval() \n",
    "        \n",
    "    def trainMode(self):\n",
    "        ## Training mode for updating parameters\n",
    "        self.actor.train()\n",
    "        self.critic1.train()\n",
    "        self.critic2.train()\n",
    "        self.targetActor.train()\n",
    "        self.targetCritic1.train() \n",
    "        self.targetCritic2.train() \n",
    "        \n",
    "    def saveModel(self, path):\n",
    "        torch.save(self.actor.state_dict(),f'{path}/actor.pkl')\n",
    "        torch.save(self.targetActor.state_dict(),f'{path}/targetActor.pkl')\n",
    "        \n",
    "        torch.save(self.critic1.state_dict(),f'{path}/critic1.pkl')\n",
    "        torch.save(self.critic2.state_dict(),f'{path}/critic2.pkl')\n",
    "        \n",
    "        torch.save(self.targetCritic1.state_dict(),f'{path}/targetCritic1.pkl')\n",
    "        torch.save(self.targetCritic2.state_dict(),f'{path}/targetCritic2.pkl')    \n",
    "        \n",
    "    def loadModel(self, path):\n",
    "        self.actor.load_state_dict(torch.load(f'{path}/actor.pkl'))\n",
    "        self.targetActor.load_state_dict(torch.load(f'{path}/targetActor.pkl'))\n",
    "        \n",
    "        self.critic1.load_state_dict(torch.load(f'{path}/critic1.pkl'))\n",
    "        self.critic2.load_state_dict(torch.load(f'{path}/critic2.pkl'))\n",
    "        \n",
    "        self.targetCritic1.load_state_dict(torch.load(f'{path}/targetCritic1.pkl'))\n",
    "        self.targetCritic2.load_state_dict(torch.load(f'{path}/targetCritic2.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdefa0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env, epochs, replayMemorySize, updateStart, printEvery = 100, savePath=None):\n",
    "\n",
    "    # ## Training Loop\n",
    "    try:\n",
    "        modelSaveCounter = 0\n",
    "        maxR = -np.inf\n",
    "        rewardTracking = []\n",
    "        rewardAvg = []\n",
    "        survival = []\n",
    "        moving = []\n",
    "        for e in range(epochs):\n",
    "            state = env.reset()    \n",
    "            episodeR = 0\n",
    "            survT = 0\n",
    "            t = 0\n",
    "            move = 0\n",
    "            # Training loop for each episode\n",
    "            while True:\n",
    "                \n",
    "                # Fill in the replay memory\n",
    "                action = agent.getAction(state)\n",
    "                try:\n",
    "                    stateNext, reward, terminated, truncated, info = env.step(action)                    \n",
    "                    agent.rMemory.addMemory((state, action, np.array([reward]), stateNext, np.array([terminated])))\n",
    "\n",
    "                    ## Update policy when memory is ready\n",
    "                    if agent.rMemory.getSize() > updateStart: \n",
    "                        agent.updatePolicy(t)\n",
    "                        t+=1\n",
    "\n",
    "                    ## renew state and action\n",
    "                    state = stateNext\n",
    "                    episodeR += reward\n",
    "                    survT += 1\n",
    "                    move += info['reward']['direction']\n",
    "                except:\n",
    "                    break\n",
    "                    \n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "            survival.append(survT)    \n",
    "            rewardTracking.append(episodeR)\n",
    "            rewardAvg.append(np.mean(rewardTracking[-10:]))\n",
    "            moving.append(move)\n",
    "            \n",
    "            if e > 5 and rewardTracking[-1] > maxR:\n",
    "                maxR = rewardTracking[-1]\n",
    "            \n",
    "                ## Only save model if reward increased\n",
    "                if savePath!=None:\n",
    "                    agent.saveModel(savePath)\n",
    "                    modelSaveCounter += 1\n",
    "                \n",
    "            if (e+1)%printEvery == 0:\n",
    "                print(f\"Episode {e+1}: reward: {episodeR}, Avg: {rewardAvg[-1]}, model improved {modelSaveCounter} times in {printEvery} episodes.\")\n",
    "                print(f\"Mean survival time: {np.mean(survival[-10:])*0.04}, mean move frd: {np.mean(moving[-10:])}\\n\")\n",
    "                modelSaveCounter = 0\n",
    "                \n",
    "    ## In case looping for too long\n",
    "    except KeyboardInterrupt:\n",
    "        return rewardTracking, rewardAvg \n",
    "    \n",
    "    return rewardTracking, rewardAvg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d815e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym_jiminy.envs import ANYmalJiminyEnv\n",
    "from gym_jiminy.common.utils import sample\n",
    "from pinocchio import neutral, normalize, framesForwardKinematics\n",
    "from jiminy_py.dynamics import compute_freeflyer_state_from_fixed_body\n",
    "from gym_jiminy.envs import ANYmalPDControlJiminyEnv\n",
    "\n",
    "## reset environment, render is not used in training\n",
    "# env = gym.make('Ant-v4', new_step_api = True)\n",
    "# env.action_space.seed(42)\n",
    "# state = env.reset()\n",
    "\n",
    "# class ConfigurableANYmal(ANYmalPDControlJiminyEnv):\n",
    "class ConfigurableANYmal(ANYmalJiminyEnv):\n",
    "    def __init__(self, reward_mixture ={'smooth':1, 'direction':1, 'done':1} ):\n",
    "        super().__init__(reward_mixture =reward_mixture )\n",
    "        self.X = 0\n",
    " \n",
    "        \n",
    "    def compute_reward(self, info):\n",
    "        # pylint: disable=arguments-differ\n",
    "\n",
    "        reward_dict = info.setdefault('reward', {})\n",
    "\n",
    "        # Define some proxies\n",
    "        reward_mixture_keys = self.reward_mixture.keys()\n",
    "\n",
    "        if 'smooth' in reward_mixture_keys:\n",
    "            reward_dict['smooth'] =  -np.sum((self._action/80)**2) * 0.2 \n",
    " \n",
    "        if 'done' in reward_mixture_keys:\n",
    "            if self.is_done():\n",
    "                reward_dict['done'] = -10\n",
    "            else:\n",
    "                reward_dict['done'] = 1\n",
    "        \n",
    "        if 'direction' in reward_mixture_keys:\n",
    "            reward_dict['direction'] =  (self.X - self.state[0][0]) / self.step_dt\n",
    " \n",
    "            self.X = self.state[0][0]\n",
    "        # Compute the total reward\n",
    "        reward_total = sum([self.reward_mixture[name] * value for name, value in reward_dict.items()])\n",
    "        return reward_total\n",
    "    \n",
    "    def _sample_state(self):\n",
    "        \n",
    "        qpos = self._neutral()\n",
    "        \n",
    "        ## introducing randomness in q\n",
    "        qpos += sample(scale=0.01, shape=(self.robot.nq,), rg=self.rg)\n",
    "        \n",
    "        # Make sure the configuration is not out-of-bound\n",
    "        pinocchio_model = self.robot.pinocchio_model\n",
    "        position_limit_lower = pinocchio_model.lowerPositionLimit\n",
    "        position_limit_upper = pinocchio_model.upperPositionLimit\n",
    "        qpos = np.minimum(np.maximum(\n",
    "            qpos, position_limit_lower), position_limit_upper)\n",
    "\n",
    "        # Make sure the configuration is normalized\n",
    "        qpos = normalize(pinocchio_model, qpos)\n",
    "\n",
    "        # Make sure the robot impacts the ground\n",
    "        if self.robot.has_freeflyer:\n",
    "            engine_options = self.simulator.engine.get_options()\n",
    "            ground_fun = engine_options['world']['groundProfile']\n",
    "            compute_freeflyer_state_from_fixed_body(\n",
    "                self.robot, qpos, ground_profile=ground_fun)\n",
    "\n",
    "        # Zero velocity\n",
    "        qvel = sample(\n",
    "            dist='normal', scale=0.01, shape=(self.robot.nv,), rg=self.rg)\n",
    "        return qpos, qvel\n",
    "\n",
    "#     def is_done(self) -> bool:  # type: ignore[override]\n",
    "#         \"\"\"Determine whether the episode is over.\n",
    "#         The termination conditions are the following:\n",
    "#             - fall detection (enabled if the robot has a freeflyer):\n",
    "#               the freeflyer goes lower than 75% of its height in\n",
    "#               neutral configuration.\n",
    "#             - maximum simulation duration exceeded\n",
    "#             - Any of the states are not valid anymore\n",
    "#         \"\"\"\n",
    "#         # pylint: disable=arguments-differ\n",
    "\n",
    "#         if not self.simulator.is_simulation_running:\n",
    "#             raise RuntimeError(\n",
    "#                 \"No simulation running. Please start one before calling this \"\n",
    "#                 \"method.\")\n",
    "#         if self.system_state.q[2] < self._height_neutral * 0.5:\n",
    "#             return True\n",
    "#         if self.simulator.stepper_state.t >= self.simu_duration_max:\n",
    "#             return True\n",
    "#         if np.sum(self.system_state.q < self.robot.position_limit_lower) or \\\n",
    "#            np.sum(self.system_state.q > self.robot.position_limit_upper):# or \\\n",
    "# #            np.sum(self.system_state.v < -self.robot.velocity_limit)      or \\\n",
    "# #            np.sum(self.system_state.v > self.robot.velocity_limit):\n",
    "#             return True\n",
    "        \n",
    "#         return False\n",
    "\n",
    "class GymWrapper:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.action_space = self.env.action_space\n",
    "        self.simulator = self.env.simulator\n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        observe, reward, terminated, info = self.env.step(action*80)\n",
    "        return self.concat(observe), reward, terminated, 0, info\n",
    "    \n",
    "    def reset(self):\n",
    "        state = self.env.reset()\n",
    "        return self.concat(state)\n",
    "    \n",
    "    @staticmethod\n",
    "    def concat(arrayA):\n",
    "        return np.concatenate([arrayA['state']['Q'], arrayA['state']['V']])\n",
    "    \n",
    "    def seed(self, value):\n",
    "        self.env.seed(value)\n",
    "        \n",
    "    def unwrap(self):\n",
    "        return self.env\n",
    "    \n",
    "    def stop(self):\n",
    "        self.env.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be50795b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linghao/env/p37_r/lib/python3.7/site-packages/ipykernel_launcher.py:61: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:204.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 250: reward: -11.4743301339453, Avg: -14.632092420817932, model improved 5 times in 250 episodes.\n",
      "Mean survival time: 0.304, mean move frd: 0.39462698215548125\n",
      "\n",
      "Episode 500: reward: -43.529240544703796, Avg: -22.589018624163685, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.648, mean move frd: -0.6747897696395022\n",
      "\n",
      "Episode 750: reward: -9.361061442466173, Avg: -10.201179206307506, model improved 1 times in 250 episodes.\n",
      "Mean survival time: 0.392, mean move frd: 0.22129256585744983\n",
      "\n",
      "Episode 1000: reward: -28.3063267340466, Avg: -12.520950284489482, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.556, mean move frd: -1.766117259944176\n",
      "\n",
      "Episode 1250: reward: -57.13341872465682, Avg: -19.644247840642795, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.5920000000000001, mean move frd: -3.6126305367885614\n",
      "\n",
      "Episode 1500: reward: -12.977493245752239, Avg: -12.834792815283489, model improved 1 times in 250 episodes.\n",
      "Mean survival time: 0.35200000000000004, mean move frd: -0.125996723256274\n",
      "\n",
      "Episode 1750: reward: -14.183001817700761, Avg: -16.668923379066545, model improved 1 times in 250 episodes.\n",
      "Mean survival time: 0.49200000000000005, mean move frd: -0.03573531349728802\n",
      "\n",
      "Episode 2000: reward: -12.955335659437155, Avg: -14.370270234593539, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.384, mean move frd: -0.1288491589942058\n",
      "\n",
      "Episode 2250: reward: -13.429266187853221, Avg: -13.742758174446795, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.348, mean move frd: -0.0047684789100937985\n",
      "\n",
      "Episode 2500: reward: -12.576356727593064, Avg: -17.014850318740237, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.41200000000000003, mean move frd: -1.7460260200569553\n",
      "\n",
      "Episode 2750: reward: -33.39695702310233, Avg: -17.194277805404948, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.40399999999999997, mean move frd: -1.3708216771748645\n",
      "\n",
      "Episode 3000: reward: -11.835472656957029, Avg: -15.46121093608335, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.336, mean move frd: 0.33397436180959506\n",
      "\n",
      "Episode 3250: reward: -22.67522089749256, Avg: -18.04263327602726, model improved 1 times in 250 episodes.\n",
      "Mean survival time: 0.44799999999999995, mean move frd: -0.09848107980681213\n",
      "\n",
      "Episode 3500: reward: -14.609606457381554, Avg: -16.503469085965868, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.43200000000000005, mean move frd: 0.8145813235648532\n",
      "\n",
      "Episode 3750: reward: -25.050930335936812, Avg: -23.20441321208093, model improved 1 times in 250 episodes.\n",
      "Mean survival time: 0.7000000000000001, mean move frd: 0.3317243812272873\n",
      "\n",
      "Episode 4000: reward: -15.917675023244449, Avg: -17.49851823469741, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.396, mean move frd: 0.4758780362886193\n",
      "\n",
      "Episode 4250: reward: 42.280125262501464, Avg: -24.819778436559723, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.756, mean move frd: 0.08578704894865723\n",
      "\n",
      "Episode 4500: reward: -8.579906103179379, Avg: -23.342650079154378, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.628, mean move frd: -0.23111693839133968\n",
      "\n",
      "Episode 4750: reward: -11.366772423541576, Avg: -22.60129854790852, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.652, mean move frd: 0.6554065382226881\n",
      "\n",
      "Episode 5000: reward: -6.860741668667695, Avg: -25.182801330689706, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.66, mean move frd: -0.1596566784376215\n",
      "\n",
      "Episode 5250: reward: -30.820127466579734, Avg: -24.301426021069137, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.7000000000000001, mean move frd: -0.31026546767768615\n",
      "\n",
      "Episode 5500: reward: -41.2572362502778, Avg: -26.37409334989622, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.728, mean move frd: -0.9212035323111106\n",
      "\n",
      "Episode 5750: reward: -18.55871805937062, Avg: -16.606158461351008, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.32, mean move frd: 0.5181723104527536\n",
      "\n",
      "Episode 6000: reward: -21.057515118458504, Avg: -22.67095772693515, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.504, mean move frd: 0.44069792331559254\n",
      "\n",
      "Episode 6250: reward: -20.14338754258282, Avg: -21.126716899132465, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.504, mean move frd: 0.44394305626480024\n",
      "\n",
      "Episode 6500: reward: 14.934186919892161, Avg: -23.679757865368597, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.588, mean move frd: -0.04150460176153814\n",
      "\n",
      "Episode 6750: reward: -27.706121812798035, Avg: -19.95691374508631, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.49600000000000005, mean move frd: 2.4589179648372874\n",
      "\n",
      "Episode 7000: reward: -22.385038562818103, Avg: -22.313538670327866, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.588, mean move frd: -0.2677135196483138\n",
      "\n",
      "Episode 7250: reward: -35.815488208570336, Avg: -28.345923767313895, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.7440000000000001, mean move frd: -1.0077565499968926\n",
      "\n",
      "Episode 7500: reward: -18.196691270143315, Avg: -19.096800367287738, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.36, mean move frd: -0.723280780962838\n",
      "\n",
      "Episode 7750: reward: -17.403204473403065, Avg: -20.724384977793857, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.40399999999999997, mean move frd: -0.6325290550515615\n",
      "\n",
      "Episode 8000: reward: -19.671277910033044, Avg: -16.37111296566986, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.376, mean move frd: 2.6666926836427463\n",
      "\n",
      "Episode 8250: reward: -17.991985118359064, Avg: -19.517195655571673, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.392, mean move frd: 0.0011816591085827776\n",
      "\n",
      "Episode 8500: reward: -22.5139258617305, Avg: -20.447562454511502, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.41200000000000003, mean move frd: -0.23154508475801389\n",
      "\n",
      "Episode 8750: reward: -22.46500090958243, Avg: -22.995760793831842, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.488, mean move frd: -0.827798413655793\n",
      "\n",
      "Episode 9000: reward: -19.72904335757662, Avg: -19.853543698386645, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.42, mean move frd: 1.054287820996509\n",
      "\n",
      "Episode 9250: reward: -17.831441242220897, Avg: -18.11001413176628, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.32, mean move frd: -0.04028942525128296\n",
      "\n",
      "Episode 9500: reward: -18.951036777016796, Avg: -18.91387305277763, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.368, mean move frd: -0.16038478038486337\n",
      "\n",
      "Episode 9750: reward: -17.44786867975287, Avg: -22.737665440159812, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.584, mean move frd: -0.04621745047219661\n",
      "\n",
      "Episode 10000: reward: -17.233213634524745, Avg: -19.358268031453598, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.396, mean move frd: -0.10158170651479201\n",
      "\n",
      "Episode 10250: reward: -18.072573684790825, Avg: -20.05742402371955, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.428, mean move frd: -0.05683539924727998\n",
      "\n",
      "Episode 10500: reward: -15.926179349733772, Avg: -18.73220640013407, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.384, mean move frd: -0.023817964373383727\n",
      "\n",
      "Episode 10750: reward: -17.809437062018556, Avg: -18.857652457935412, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.36, mean move frd: -0.5386045583421205\n",
      "\n",
      "Episode 11000: reward: -16.97637929999744, Avg: -18.22219266884886, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.35200000000000004, mean move frd: 0.10323357943383393\n",
      "\n",
      "Episode 11250: reward: -18.95675085832941, Avg: -19.141182304648282, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.40399999999999997, mean move frd: -0.3069014122123357\n",
      "\n",
      "Episode 11500: reward: -30.31834268591402, Avg: -21.437470256623165, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.49200000000000005, mean move frd: 0.21584482440636482\n",
      "\n",
      "Episode 11750: reward: -19.3722328537331, Avg: -18.466796577463708, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.35600000000000004, mean move frd: 0.06443804242680236\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 12000: reward: -18.344226903117463, Avg: -18.563078165430703, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.35600000000000004, mean move frd: 0.06388053733361791\n",
      "\n",
      "Episode 12250: reward: -19.845441685006698, Avg: -18.32341791962705, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.35200000000000004, mean move frd: 0.04834114439157953\n",
      "\n",
      "Episode 12500: reward: -16.367025366746077, Avg: -18.720318239106792, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.376, mean move frd: 0.1246487393220165\n",
      "\n",
      "Episode 12750: reward: -15.627933867142765, Avg: -17.1257985732593, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.36, mean move frd: 0.7868305835560356\n",
      "\n",
      "Episode 13000: reward: -18.88005083802677, Avg: -18.677458604105958, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.38, mean move frd: 0.29438843831785616\n",
      "\n",
      "Episode 13250: reward: -17.655568566755917, Avg: -19.012319789903966, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.408, mean move frd: 0.0672477143157548\n",
      "\n",
      "Episode 13500: reward: -15.754857447852546, Avg: -16.770285880331958, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.324, mean move frd: -0.04005147989485443\n",
      "\n",
      "Episode 13750: reward: -15.402823629508713, Avg: -17.015241891282137, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.36, mean move frd: 1.1241615939279557\n",
      "\n",
      "Episode 14000: reward: -16.42124775893378, Avg: -18.8653065999187, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.44799999999999995, mean move frd: 0.12177281739831662\n",
      "\n",
      "Episode 14250: reward: -17.496268272562848, Avg: -17.611868926054477, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.36, mean move frd: -0.18579082728517693\n",
      "\n",
      "Episode 14500: reward: -15.136429761282677, Avg: -17.682403382625527, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.36, mean move frd: 0.18376544541779843\n",
      "\n",
      "Episode 14750: reward: -17.58688465149863, Avg: -17.483408594946994, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.37200000000000005, mean move frd: -0.11030702941212318\n",
      "\n",
      "Episode 15000: reward: -14.800412450771777, Avg: -18.08653570496039, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.368, mean move frd: 0.04209968835585829\n",
      "\n",
      "Episode 15250: reward: -15.725224031048823, Avg: -18.73810760778606, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.444, mean move frd: 0.008154902825805822\n",
      "\n",
      "Episode 15500: reward: -18.967572849883986, Avg: -18.24893621809523, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.38799999999999996, mean move frd: 0.08172452245270438\n",
      "\n",
      "Episode 15750: reward: -16.74211145039518, Avg: -16.816814584883396, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.32, mean move frd: 0.056442123510123246\n",
      "\n",
      "Episode 16000: reward: -17.582062503357474, Avg: -17.532418606887383, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.384, mean move frd: 0.1520406819798613\n",
      "\n",
      "Episode 16250: reward: -19.474595987619026, Avg: -19.87161628450098, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.46799999999999997, mean move frd: 0.5392261339383346\n",
      "\n",
      "Episode 16500: reward: -17.928235566473038, Avg: -18.029226423204975, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.364, mean move frd: -0.09269019534414238\n",
      "\n",
      "Episode 16750: reward: -15.269267506339839, Avg: -16.17069586653297, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.332, mean move frd: 0.12324667432074783\n",
      "\n",
      "Episode 17000: reward: -16.65344866170092, Avg: -16.715466737280952, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.376, mean move frd: 0.06640659641149148\n",
      "\n",
      "Episode 17250: reward: -17.970387283579846, Avg: -16.73005593481202, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.34, mean move frd: -0.038831238958494364\n",
      "\n",
      "Episode 17500: reward: -15.029441865725719, Avg: -15.932418562820596, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.332, mean move frd: 0.050579586010046564\n",
      "\n",
      "Episode 17750: reward: -14.794402311673311, Avg: -15.247602742501527, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.28800000000000003, mean move frd: 0.033598410221170556\n",
      "\n",
      "Episode 18000: reward: -14.65954004373656, Avg: -15.079802017269367, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.324, mean move frd: -0.008790556955604637\n",
      "\n",
      "Episode 18250: reward: -14.221934132795637, Avg: -15.134220881495295, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.308, mean move frd: 0.09541824151542816\n",
      "\n",
      "Episode 18500: reward: -14.894711865784998, Avg: -16.804756137732074, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.46399999999999997, mean move frd: 0.09688761574327355\n",
      "\n",
      "Episode 18750: reward: -14.00367117569778, Avg: -15.250587616779944, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.336, mean move frd: 0.07396287397943413\n",
      "\n",
      "Episode 19000: reward: -20.58924989231697, Avg: -16.37942895691457, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.42, mean move frd: -0.4908088642063017\n",
      "\n",
      "Episode 19250: reward: -14.491599872816401, Avg: -16.350843250614652, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.4, mean move frd: 0.08250960945518719\n",
      "\n",
      "Episode 19500: reward: -18.852722199247363, Avg: -16.327299855433655, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.444, mean move frd: -0.03556033929488915\n",
      "\n",
      "Episode 19750: reward: -30.978416615281915, Avg: -20.85874939334895, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.64, mean move frd: 0.4732683397848122\n",
      "\n",
      "Episode 20000: reward: -20.353682202108857, Avg: -16.8689468897666, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.34, mean move frd: 0.014386325193471983\n",
      "\n",
      "Episode 20250: reward: -26.898391225786398, Avg: -22.677057543431637, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.628, mean move frd: -0.12778281026506014\n",
      "\n",
      "Episode 20500: reward: -18.153489992891608, Avg: -17.180597762741918, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.428, mean move frd: 0.22690610575089262\n",
      "\n",
      "Episode 20750: reward: -18.05677620611325, Avg: -17.298068501458765, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.332, mean move frd: -0.49105231682486067\n",
      "\n",
      "Episode 21000: reward: -18.487889814505202, Avg: -17.45862332383668, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.41200000000000003, mean move frd: -0.1924475523818796\n",
      "\n",
      "Episode 21250: reward: -15.691851234130745, Avg: -16.887467009263133, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.436, mean move frd: -0.060404098782658774\n",
      "\n",
      "Episode 21500: reward: -20.046877922831463, Avg: -18.524738852590417, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.32, mean move frd: -0.10104185750836492\n",
      "\n",
      "Episode 21750: reward: -17.21499421729199, Avg: -20.992257801159095, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.512, mean move frd: -0.4555632579953953\n",
      "\n",
      "Episode 22000: reward: -21.59942380405458, Avg: -20.601498032516808, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.568, mean move frd: 0.4302902660296265\n",
      "\n",
      "Episode 22250: reward: -15.784086734274746, Avg: -20.21848762246546, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.544, mean move frd: -0.39793700838217005\n",
      "\n",
      "Episode 22500: reward: -15.56308661753179, Avg: -16.18610189366455, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.32799999999999996, mean move frd: -0.16874790506163745\n",
      "\n",
      "Episode 22750: reward: -15.49965229821166, Avg: -22.293453606145015, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.636, mean move frd: -0.7067801300037425\n",
      "\n",
      "Episode 23000: reward: -31.094067384653115, Avg: -24.184883555934256, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.68, mean move frd: -0.22473103046774368\n",
      "\n",
      "Episode 23250: reward: -20.65813190484566, Avg: -20.064153554324026, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.47600000000000003, mean move frd: -0.04026762860260966\n",
      "\n",
      "Episode 23500: reward: -16.814826117612483, Avg: -17.915574096427637, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.348, mean move frd: 0.05425080461032309\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 23750: reward: -29.935429611337455, Avg: -18.70739482320015, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.392, mean move frd: -0.26932373228711165\n",
      "\n",
      "Episode 24000: reward: -21.381808439338975, Avg: -24.712918084128138, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.792, mean move frd: -0.05453740339577066\n",
      "\n",
      "Episode 24250: reward: -18.483136645323913, Avg: -17.030853049649117, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.344, mean move frd: -0.10457147591028344\n",
      "\n",
      "Episode 24500: reward: -16.785938331316736, Avg: -19.243119375075477, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.46399999999999997, mean move frd: 0.05507362504649249\n",
      "\n",
      "Episode 24750: reward: -11.954712889757463, Avg: -21.413209393524962, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.588, mean move frd: -0.6245489080096701\n",
      "\n",
      "Episode 25000: reward: -17.953870131741027, Avg: -17.34634837784744, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.336, mean move frd: -0.4248398041999081\n",
      "\n",
      "Episode 25250: reward: -16.84563658899551, Avg: -16.169352882444052, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.332, mean move frd: -0.013869792400716818\n",
      "\n",
      "Episode 25500: reward: -15.640736552439943, Avg: -16.509436514381342, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.364, mean move frd: -0.0955646570225154\n",
      "\n",
      "Episode 25750: reward: -16.973960266173343, Avg: -15.820376594851387, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.32799999999999996, mean move frd: -0.10746659395479481\n",
      "\n",
      "Episode 26000: reward: -13.832893152731852, Avg: -15.552897738506687, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.348, mean move frd: 0.05475402523237235\n",
      "\n",
      "Episode 26250: reward: -17.027750965591142, Avg: -16.87735324462465, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.396, mean move frd: -0.026225138395459435\n",
      "\n",
      "Episode 26500: reward: -20.121538302442026, Avg: -18.637950209511242, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.49200000000000005, mean move frd: -0.16806158369838592\n",
      "\n",
      "Episode 26750: reward: -9.231605491425158, Avg: -18.11320056133974, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.516, mean move frd: 0.01072601292754527\n",
      "\n",
      "Episode 27000: reward: -10.400614099298334, Avg: -14.849031014229581, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.336, mean move frd: 0.7761583567118333\n",
      "\n",
      "Episode 27250: reward: -10.76106999664838, Avg: -17.788666390678394, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.516, mean move frd: 0.3531077896692649\n",
      "\n",
      "Episode 27500: reward: -16.801487292631876, Avg: -15.956345387591728, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.38, mean move frd: -0.15629817696153842\n",
      "\n",
      "Episode 27750: reward: -13.09041172439396, Avg: -15.017477281112301, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.332, mean move frd: 0.22327455371124127\n",
      "\n",
      "Episode 28000: reward: -14.817747927942987, Avg: -17.057574533990795, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.49600000000000005, mean move frd: 0.03232462064680421\n",
      "\n",
      "Episode 28250: reward: -16.847982853421144, Avg: -16.08691685497809, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.36, mean move frd: -0.08490497534414566\n",
      "\n",
      "Episode 28500: reward: -14.345740673431738, Avg: -18.21649970814615, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.528, mean move frd: -0.16881929639988452\n",
      "\n",
      "Episode 28750: reward: -19.81247358049596, Avg: -15.983295460302173, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.4, mean move frd: 0.04713978856682699\n",
      "\n",
      "Episode 29000: reward: -17.899107940678082, Avg: -16.84464418649166, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.43200000000000005, mean move frd: 0.08633407531764395\n",
      "\n",
      "Episode 29250: reward: -15.382935703290793, Avg: -15.144818950145108, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.36, mean move frd: -0.05180586585905315\n",
      "\n",
      "Episode 29500: reward: -14.008122863394812, Avg: -15.010180168219062, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.336, mean move frd: 0.062078951745952946\n",
      "\n",
      "Episode 29750: reward: -13.25612792628954, Avg: -15.83496787276869, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.41600000000000004, mean move frd: -0.11618049090487088\n",
      "\n",
      "Episode 30000: reward: -13.84303890036825, Avg: -14.945788955708029, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.38, mean move frd: 0.07971036175877846\n",
      "\n",
      "Episode 30250: reward: -13.074281587579193, Avg: -15.176880710283609, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.38799999999999996, mean move frd: 0.21353504384596142\n",
      "\n",
      "Episode 30500: reward: -9.262633225699341, Avg: -16.80281154508678, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.49200000000000005, mean move frd: 0.02741696049872653\n",
      "\n",
      "Episode 30750: reward: -15.727699963128694, Avg: -15.394023746590682, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.392, mean move frd: -0.0391577352631217\n",
      "\n",
      "Episode 31000: reward: -13.87816164983835, Avg: -16.658566139163312, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.48, mean move frd: 0.22591891624035904\n",
      "\n",
      "Episode 31250: reward: -21.3022413689751, Avg: -16.871326932326575, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.484, mean move frd: -0.10898356318606628\n",
      "\n",
      "Episode 31500: reward: -12.026585170413671, Avg: -15.313563432761494, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.43200000000000005, mean move frd: 0.3410300324577934\n",
      "\n",
      "Episode 31750: reward: -13.831759524927012, Avg: -15.233258338200713, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.38, mean move frd: 0.1925887993674528\n",
      "\n",
      "Episode 32000: reward: -15.147333543010726, Avg: -15.122782741778874, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.41200000000000003, mean move frd: 0.24915018044291593\n",
      "\n",
      "Episode 32250: reward: -14.721656678901951, Avg: -15.562755904226496, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.46799999999999997, mean move frd: 0.1474827177823808\n",
      "\n",
      "Episode 32500: reward: -21.194374758641825, Avg: -16.972989393483594, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.536, mean move frd: 0.18389254487256176\n",
      "\n",
      "Episode 32750: reward: -18.775418974426962, Avg: -17.619367451233565, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.544, mean move frd: -0.39041783153219567\n",
      "\n",
      "Episode 33000: reward: -16.272103873773215, Avg: -18.15203834815137, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.56, mean move frd: -0.1802108929761001\n",
      "\n",
      "Episode 33250: reward: -15.466182927014906, Avg: -17.478378942550318, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.524, mean move frd: -0.05889105685756424\n",
      "\n",
      "Episode 33500: reward: -20.02327417799806, Avg: -17.090969124697956, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.48, mean move frd: 0.2589876645751561\n",
      "\n",
      "Episode 33750: reward: -14.248093857609883, Avg: -15.827726303259789, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.336, mean move frd: -0.1339359614328356\n",
      "\n",
      "Episode 34000: reward: -11.359868165785135, Avg: -18.503939282128925, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.648, mean move frd: 0.4143408077676577\n",
      "\n",
      "Episode 34250: reward: -16.33360023395922, Avg: -17.31072535639265, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.484, mean move frd: -0.19447924755398116\n",
      "\n",
      "Episode 34500: reward: -14.645605209000585, Avg: -15.349961026912037, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.46799999999999997, mean move frd: 1.2569022640945695\n",
      "\n",
      "Episode 34750: reward: -16.284930643193427, Avg: -16.007334134648598, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.324, mean move frd: 0.06732625200481528\n",
      "\n",
      "Episode 35000: reward: -23.798857630440313, Avg: -16.980687352898503, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.46399999999999997, mean move frd: -0.3522091091389571\n",
      "\n",
      "Episode 35250: reward: -16.20858099917638, Avg: -15.678022212724326, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.38799999999999996, mean move frd: 0.4247316727496207\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 35500: reward: -15.302890305172987, Avg: -16.17942725117326, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.40399999999999997, mean move frd: -0.010872897215500882\n",
      "\n",
      "Episode 35750: reward: -20.24506077358491, Avg: -16.818799906277967, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.428, mean move frd: -0.0629269309856975\n",
      "\n",
      "Episode 36000: reward: -16.08244965514547, Avg: -17.737666582793217, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.49600000000000005, mean move frd: 0.030441453166496123\n",
      "\n",
      "Episode 36250: reward: -19.701976756112465, Avg: -17.259731682003242, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.544, mean move frd: -0.15901312612129456\n",
      "\n",
      "Episode 36500: reward: -20.67585202264162, Avg: -16.92005876530851, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.436, mean move frd: -0.02467560814585923\n",
      "\n",
      "Episode 36750: reward: -21.39645014266953, Avg: -18.66837025567837, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.568, mean move frd: -0.22766066031674703\n",
      "\n",
      "Episode 37000: reward: -19.985953374903083, Avg: -17.059379209243545, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.43200000000000005, mean move frd: -0.2287422648460243\n",
      "\n",
      "Episode 37250: reward: -12.026656870480819, Avg: -14.847107786392659, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.408, mean move frd: 0.28530440794415124\n",
      "\n",
      "Episode 37500: reward: -15.214348440693588, Avg: -16.38187857086306, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.5479999999999999, mean move frd: -0.08323515696395098\n",
      "\n",
      "Episode 37750: reward: -16.067898729808178, Avg: -15.57925972578675, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.35200000000000004, mean move frd: -0.07270885943500924\n",
      "\n",
      "Episode 38000: reward: -30.830452026047936, Avg: -18.562436561883892, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.628, mean move frd: -0.29119239738002245\n",
      "\n",
      "Episode 38250: reward: -15.683536573140493, Avg: -15.688721197194337, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.38799999999999996, mean move frd: -0.16265000334715726\n",
      "\n",
      "Episode 38500: reward: -22.94191884775354, Avg: -17.83824107969023, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.624, mean move frd: 1.2029857053306166\n",
      "\n",
      "Episode 38750: reward: -18.187943995881284, Avg: -17.594335028261334, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.47200000000000003, mean move frd: -0.5710689839529584\n",
      "\n",
      "Episode 39000: reward: -19.681372495934966, Avg: -16.85577158690095, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.49200000000000005, mean move frd: -0.675060307773788\n",
      "\n",
      "Episode 39250: reward: -18.068773894701238, Avg: -16.98201574571699, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.512, mean move frd: -0.2557900648832948\n",
      "\n",
      "Episode 39500: reward: -13.773145917453412, Avg: -16.2661590265716, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.43200000000000005, mean move frd: -0.26810876343656015\n",
      "\n",
      "Episode 39750: reward: -20.699743173976834, Avg: -17.54944958868149, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.54, mean move frd: -0.4066467271129218\n",
      "\n",
      "Episode 40000: reward: -17.061724578731578, Avg: -16.347721295033974, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.4, mean move frd: 0.17379835427078796\n",
      "\n",
      "Episode 40250: reward: -16.369459624145666, Avg: -16.22390553507264, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.42, mean move frd: -0.01107216798393329\n",
      "\n",
      "Episode 40500: reward: -21.76434074284342, Avg: -17.988408387718856, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.596, mean move frd: -0.1025996304672255\n",
      "\n",
      "Episode 40750: reward: -6.410340284374308, Avg: -17.253329423662716, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.568, mean move frd: -0.10971515194241484\n",
      "\n",
      "Episode 41000: reward: -12.469369319844033, Avg: -14.627795816313741, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.38, mean move frd: 0.2888786201012023\n",
      "\n",
      "Episode 41250: reward: -12.62890937662582, Avg: -15.712715500301822, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.42, mean move frd: -0.09011308529349008\n",
      "\n",
      "Episode 41500: reward: -19.88638962054715, Avg: -17.532443393714562, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.54, mean move frd: -0.20716922799831855\n",
      "\n",
      "Episode 41750: reward: -14.905830020101172, Avg: -18.518511239125907, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.616, mean move frd: 0.7320172504602034\n",
      "\n",
      "Episode 42000: reward: -24.97500415949096, Avg: -18.341235361091695, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.664, mean move frd: -0.48872116856977643\n",
      "\n",
      "Episode 42250: reward: -24.973993074981294, Avg: -16.07244499713513, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.40399999999999997, mean move frd: -0.4880814916877808\n",
      "\n",
      "Episode 42500: reward: -21.03588213701618, Avg: -17.88259329271182, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.56, mean move frd: -0.332382974613964\n",
      "\n",
      "Episode 42750: reward: -15.75964452861961, Avg: -15.897184796171809, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.54, mean move frd: -0.038685034183787895\n",
      "\n",
      "Episode 43000: reward: -14.502210411405958, Avg: -18.333738706324507, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.7000000000000001, mean move frd: 0.20200490778852168\n",
      "\n",
      "Episode 43250: reward: -26.02686748966214, Avg: -19.466891868312107, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.728, mean move frd: 1.2117481196205668\n",
      "\n",
      "Episode 43500: reward: -21.53476972087922, Avg: -18.74387888155908, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.64, mean move frd: -0.3878648657096729\n",
      "\n",
      "Episode 43750: reward: -16.386827448828527, Avg: -17.611474193654914, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.44, mean move frd: 0.07327447148218376\n",
      "\n",
      "Episode 44000: reward: -16.64153276373095, Avg: -15.910745019830653, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.392, mean move frd: -0.0766597300642927\n",
      "\n",
      "Episode 44250: reward: -16.716326888668295, Avg: -15.233266721500897, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.46, mean move frd: 1.7808651786392145\n",
      "\n",
      "Episode 44500: reward: -14.39924676505382, Avg: -14.57791438389342, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.364, mean move frd: -0.016679051576308817\n",
      "\n",
      "Episode 44750: reward: -21.431574996768113, Avg: -17.094529421345335, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.5760000000000001, mean move frd: -0.5516974886866333\n",
      "\n",
      "Episode 45000: reward: -13.298324824584341, Avg: -14.53425045589511, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.35600000000000004, mean move frd: 0.01710992520041168\n",
      "\n",
      "Episode 45250: reward: -14.171804646862856, Avg: -14.358969569142815, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.368, mean move frd: -0.14895326876680393\n",
      "\n",
      "Episode 45500: reward: -17.495931984607147, Avg: -14.332525056569594, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.368, mean move frd: -0.14045551054462524\n",
      "\n",
      "Episode 45750: reward: -18.12052435494191, Avg: -16.095293848298702, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.49600000000000005, mean move frd: 0.035659673744072286\n",
      "\n",
      "Episode 46000: reward: -12.790387347253615, Avg: -14.760920956782874, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.49600000000000005, mean move frd: 0.4187076733282836\n",
      "\n",
      "Episode 46250: reward: -13.813403611329626, Avg: -14.276280079876837, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.408, mean move frd: 0.030345957313053274\n",
      "\n",
      "Episode 46500: reward: -17.216128310470772, Avg: -15.278975467717853, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.43200000000000005, mean move frd: -0.0351366936325644\n",
      "\n",
      "Episode 46750: reward: -26.840056652945226, Avg: -18.015294744130273, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.52, mean move frd: -0.23464507909804017\n",
      "\n",
      "Episode 47000: reward: -17.723199150967964, Avg: -19.1287945276897, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.58, mean move frd: 0.1489960691304741\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 47250: reward: -19.734312360613146, Avg: -17.122991349177376, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.504, mean move frd: 0.13237704210584086\n",
      "\n",
      "Episode 47500: reward: -20.114376950542315, Avg: -20.04476422917482, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.524, mean move frd: 0.04291517347686258\n",
      "\n",
      "Episode 47750: reward: -18.38017626481272, Avg: -19.81459281777567, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.536, mean move frd: -0.08345502013146064\n",
      "\n",
      "Episode 48000: reward: -20.835732107545745, Avg: -17.584537034731447, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.436, mean move frd: 0.01166711734636885\n",
      "\n",
      "Episode 48250: reward: -17.383120380104067, Avg: -17.23616371531407, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.46, mean move frd: 0.09295642914440465\n",
      "\n",
      "Episode 48500: reward: -15.336474628935678, Avg: -17.286194305417727, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.436, mean move frd: -0.314494135319188\n",
      "\n",
      "Episode 48750: reward: -14.446191495667065, Avg: -16.731982966096986, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.436, mean move frd: -0.19022150486580353\n",
      "\n",
      "Episode 49000: reward: -14.247634213597172, Avg: -16.60521803455508, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.44, mean move frd: 0.07208406770209161\n",
      "\n",
      "Episode 49250: reward: -17.232053539411723, Avg: -16.288660461867845, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.41600000000000004, mean move frd: 0.0643790207495634\n",
      "\n",
      "Episode 49500: reward: -14.517210423537977, Avg: -15.230983300848155, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.444, mean move frd: -0.10838788455080721\n",
      "\n",
      "Episode 49750: reward: -17.123054395461914, Avg: -16.197035206589113, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.49200000000000005, mean move frd: -0.16982173050557964\n",
      "\n",
      "Episode 50000: reward: -12.74492308753915, Avg: -19.128320271520685, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.7120000000000001, mean move frd: 0.23771629012457468\n",
      "\n",
      "Episode 50250: reward: -18.784639122453278, Avg: -16.29541780011662, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.536, mean move frd: -0.16851284381245518\n",
      "\n",
      "Episode 50500: reward: -17.810138303628616, Avg: -16.792622808331444, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.504, mean move frd: -0.11116821102993776\n",
      "\n",
      "Episode 50750: reward: -18.85535521161815, Avg: -18.786549069372228, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.54, mean move frd: -0.9096866911828254\n",
      "\n",
      "Episode 51000: reward: -15.577303593335785, Avg: -17.527849615130016, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.56, mean move frd: 0.1200191667544385\n",
      "\n",
      "Episode 51250: reward: -15.32767248997613, Avg: -17.62743680617318, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.536, mean move frd: 0.17260016277496365\n",
      "\n",
      "Episode 51500: reward: -17.132392800587887, Avg: -19.20380318028153, model improved 1 times in 250 episodes.\n",
      "Mean survival time: 0.58, mean move frd: -0.2360820331172225\n",
      "\n",
      "Episode 51750: reward: -14.92324595436488, Avg: -14.572056794019062, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.3, mean move frd: -0.06908422148348878\n",
      "\n",
      "Episode 52000: reward: -14.061472613671416, Avg: -15.985562430597142, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.40399999999999997, mean move frd: -0.24100516245253711\n",
      "\n",
      "Episode 52250: reward: -15.444037120131226, Avg: -16.021114667163026, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.428, mean move frd: -0.08178591781314629\n",
      "\n",
      "Episode 52500: reward: -13.188736875259977, Avg: -16.3263018499565, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.42, mean move frd: -0.2771975254560542\n",
      "\n",
      "Episode 52750: reward: -16.10943958434595, Avg: -16.909249479337262, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.5720000000000001, mean move frd: 0.1946322379972039\n",
      "\n",
      "Episode 53000: reward: -13.523802517057952, Avg: -15.573496382350507, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.5479999999999999, mean move frd: 0.2627783230465287\n",
      "\n",
      "Episode 53250: reward: -29.41365498100661, Avg: -18.86494059755324, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.7000000000000001, mean move frd: -0.7482160421673798\n",
      "\n",
      "Episode 53500: reward: -15.38804454469432, Avg: -17.53968216746016, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.716, mean move frd: 2.219756436092144\n",
      "\n",
      "Episode 53750: reward: -14.747215000931762, Avg: -13.416370063449728, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.368, mean move frd: -0.006379569711231647\n",
      "\n",
      "Episode 54000: reward: -9.846335597631997, Avg: -17.946292794221527, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.532, mean move frd: 0.46246632297460577\n",
      "\n",
      "Episode 54250: reward: -9.565911996261615, Avg: -15.900539692978729, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.488, mean move frd: 0.5856166485697769\n",
      "\n",
      "Episode 54500: reward: -17.702364462506807, Avg: -18.1703016112346, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.4, mean move frd: -0.6555117572987581\n",
      "\n",
      "Episode 54750: reward: -32.56274450731369, Avg: -16.181587008538298, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.452, mean move frd: -0.05852623176003853\n",
      "\n",
      "Episode 55000: reward: -15.623197820588096, Avg: -17.011682430233808, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.564, mean move frd: -1.3033358204528889\n",
      "\n",
      "Episode 55250: reward: -26.292680538070464, Avg: -13.954981575275372, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.5720000000000001, mean move frd: 2.238673552002438\n",
      "\n",
      "Episode 55500: reward: -14.781160937067197, Avg: -14.00791390885872, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.46, mean move frd: 0.050833890517490876\n",
      "\n",
      "Episode 55750: reward: -17.078980115562146, Avg: -17.714342737128597, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.5479999999999999, mean move frd: 0.13330969192926642\n",
      "\n",
      "Episode 56000: reward: -16.20850766062491, Avg: -17.77698684653793, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.516, mean move frd: -0.5022980453603008\n",
      "\n",
      "Episode 56250: reward: -14.27602742339668, Avg: -15.413356848342655, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.444, mean move frd: -0.2581210460145259\n",
      "\n",
      "Episode 56500: reward: -15.408420807227856, Avg: -17.105513650733577, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.604, mean move frd: -0.14092151125848096\n",
      "\n",
      "Episode 56750: reward: -25.250830942199585, Avg: -17.405775334916143, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.6559999999999999, mean move frd: -0.09203423478383455\n",
      "\n",
      "Episode 57000: reward: -21.143882616555416, Avg: -14.136007620544637, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.44, mean move frd: -0.811878927141341\n",
      "\n",
      "Episode 57250: reward: -16.405111384672892, Avg: -17.55088479542323, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.664, mean move frd: 0.04860730281336532\n",
      "\n",
      "Episode 57500: reward: -37.764777499245646, Avg: -18.8429665729552, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.544, mean move frd: -1.7859824726685745\n",
      "\n",
      "Episode 57750: reward: -11.52488134980356, Avg: -14.966500621585405, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.4, mean move frd: -0.3321328746796043\n",
      "\n",
      "Episode 58000: reward: -9.810028162261649, Avg: -19.03334262395193, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.74, mean move frd: 1.0246548089671947\n",
      "\n",
      "Episode 58250: reward: -19.64519902265969, Avg: -15.686590748081452, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.56, mean move frd: -0.09024759517730405\n",
      "\n",
      "Episode 58500: reward: -15.38848450078774, Avg: -15.580981320243476, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.456, mean move frd: -0.6349712441918125\n",
      "\n",
      "Episode 58750: reward: -15.801161189616593, Avg: -15.410578266411221, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.46399999999999997, mean move frd: 0.04134916167742484\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 59000: reward: -15.967418652014185, Avg: -15.879523316649957, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.5760000000000001, mean move frd: -0.0924955611497494\n",
      "\n",
      "Episode 59250: reward: -24.52693910557825, Avg: -15.84997168645736, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.616, mean move frd: 0.05354044978794992\n",
      "\n",
      "Episode 59500: reward: -10.482353207177828, Avg: -15.122561011835703, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.7120000000000001, mean move frd: 0.9995000813340624\n",
      "\n",
      "Episode 59750: reward: -22.593822322998285, Avg: -15.384853371603635, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.528, mean move frd: -0.8196027194884714\n",
      "\n",
      "Episode 60000: reward: -13.916993106782943, Avg: -15.110042687182297, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.532, mean move frd: 0.6841229202945895\n",
      "\n",
      "Episode 60250: reward: -4.464857426138191, Avg: -12.761195444442254, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.512, mean move frd: 1.0482361226644212\n",
      "\n",
      "Episode 60500: reward: -15.096476343110911, Avg: -14.42007402253285, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.484, mean move frd: -0.11547911466899277\n",
      "\n",
      "Episode 60750: reward: -22.52502293727407, Avg: -16.521764601605728, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.564, mean move frd: 0.06321748187963791\n",
      "\n",
      "Episode 61000: reward: -20.970093567426055, Avg: -16.321659131556746, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.66, mean move frd: -0.2899020780547999\n",
      "\n",
      "Episode 61250: reward: -20.3141903644398, Avg: -18.600999935950476, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.792, mean move frd: 0.12421649527559468\n",
      "\n",
      "Episode 61500: reward: -19.026256561657846, Avg: -17.41228891208287, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.616, mean move frd: -0.06768946404297256\n",
      "\n",
      "Episode 61750: reward: -1.1513147304570825, Avg: -23.354965874142117, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.9520000000000001, mean move frd: -0.7022636371083741\n",
      "\n",
      "Episode 62000: reward: -13.215513122824301, Avg: -14.801002841573084, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.41200000000000003, mean move frd: -0.007382977921264189\n",
      "\n",
      "Episode 62250: reward: -16.29396584758484, Avg: -15.245812190004, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.46, mean move frd: -0.18989065902423796\n",
      "\n",
      "Episode 62500: reward: -13.280252544083696, Avg: -13.723416629922705, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.38, mean move frd: 0.0650295736107625\n",
      "\n",
      "Episode 62750: reward: -16.02780390126438, Avg: -14.495783149729785, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.424, mean move frd: -0.21853490405670292\n",
      "\n",
      "Episode 63000: reward: -14.201203517162382, Avg: -14.332756752709837, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.41600000000000004, mean move frd: 0.030606355142437758\n",
      "\n",
      "Episode 63250: reward: -13.305081649480407, Avg: -13.510528281975136, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.376, mean move frd: 0.09210251886768213\n",
      "\n",
      "Episode 63500: reward: -16.61325287510923, Avg: -13.870732928697786, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.42, mean move frd: 0.01040175463474502\n",
      "\n",
      "Episode 63750: reward: -11.0197913450421, Avg: -13.289610722849469, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.456, mean move frd: 0.25773715839558464\n",
      "\n",
      "Episode 64000: reward: -10.107531769719897, Avg: -12.906263884016132, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.46, mean move frd: 0.36502666442119025\n",
      "\n",
      "Episode 64250: reward: -14.45880458599696, Avg: -13.54423327747252, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.452, mean move frd: -0.07853491861596351\n",
      "\n",
      "Episode 64500: reward: -15.378752239335618, Avg: -14.665129897146752, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.41600000000000004, mean move frd: -0.43250065675133287\n",
      "\n",
      "Episode 64750: reward: -18.49513838778877, Avg: -14.034684506837598, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.5, mean move frd: -0.5257122680406009\n",
      "\n",
      "Episode 65000: reward: -13.044473361604599, Avg: -12.548273595902659, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.484, mean move frd: 0.07115258963675242\n",
      "\n",
      "Episode 65250: reward: -13.453051512633767, Avg: -13.070135617582133, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.43200000000000005, mean move frd: -0.304207685554684\n",
      "\n",
      "Episode 65500: reward: -5.775439696704128, Avg: -14.361910085427164, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.556, mean move frd: 0.6619419139066995\n",
      "\n",
      "Episode 65750: reward: -14.640059318687387, Avg: -14.66026149159291, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.436, mean move frd: 0.054255336752903706\n",
      "\n",
      "Episode 66000: reward: -16.07167017817438, Avg: -13.835944962395264, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.5, mean move frd: 0.273067905309375\n",
      "\n",
      "Episode 66250: reward: -12.382054016050247, Avg: -13.211771296129317, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.512, mean move frd: -0.01895256688456534\n",
      "\n",
      "Episode 66500: reward: -14.158286246539616, Avg: -13.867764266835284, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.604, mean move frd: 0.0713063827360967\n",
      "\n",
      "Episode 66750: reward: -19.191877529352354, Avg: -15.070078997511946, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.66, mean move frd: 0.14295890054538196\n",
      "\n",
      "Episode 67000: reward: -11.732631293222026, Avg: -13.103968331824381, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.436, mean move frd: 0.08596671862353597\n",
      "\n",
      "Episode 67250: reward: 20.045424476214198, Avg: -11.901978478630069, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.608, mean move frd: 2.6582794630271076\n",
      "\n",
      "Episode 67500: reward: -12.498954012844694, Avg: -13.217939704848083, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.424, mean move frd: -0.16134967904270017\n",
      "\n",
      "Episode 67750: reward: -13.115916329223117, Avg: -13.680846285614518, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.436, mean move frd: 0.7067283990505268\n",
      "\n",
      "Episode 68000: reward: -9.732179844350894, Avg: -15.596106465203354, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.564, mean move frd: 0.48021301644218395\n",
      "\n",
      "Episode 68250: reward: -14.646647282105343, Avg: -13.890160839233701, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.42, mean move frd: 0.03879660774001592\n",
      "\n",
      "Episode 68500: reward: -22.727703670523717, Avg: -14.661333462054733, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.44799999999999995, mean move frd: -0.2092999289710858\n",
      "\n",
      "Episode 68750: reward: -15.150298583575385, Avg: -12.698844961382884, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.48, mean move frd: 0.484219611635243\n",
      "\n",
      "Episode 69000: reward: -20.08092755482529, Avg: -15.589350090694222, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.828, mean move frd: 0.5380141891544612\n",
      "\n",
      "Episode 69250: reward: -3.1288230498097374, Avg: -14.819501860370886, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.608, mean move frd: 2.207129945386053\n",
      "\n",
      "Episode 69500: reward: -20.76764736307593, Avg: -20.029712590251403, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.816, mean move frd: 0.49405401619214206\n",
      "\n",
      "Episode 69750: reward: -3.6511303018565755, Avg: -12.434859821956072, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.46799999999999997, mean move frd: 1.4616952213339682\n",
      "\n",
      "Episode 70000: reward: -19.885548252400536, Avg: -16.21058476912535, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.62, mean move frd: 0.6034594006864976\n",
      "\n",
      "Episode 70250: reward: -24.087272672281678, Avg: -18.77683838031837, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.6759999999999999, mean move frd: 0.16623523499037596\n",
      "\n",
      "Episode 70500: reward: -13.647777755669505, Avg: -19.912395328430854, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.636, mean move frd: -0.3987918191313627\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 70750: reward: -16.44846668550474, Avg: -15.587338317621692, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.44799999999999995, mean move frd: 0.2383701118169393\n",
      "\n",
      "Episode 71000: reward: -20.757670038213682, Avg: -17.076059352581726, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.47200000000000003, mean move frd: 0.04618407390959971\n",
      "\n",
      "Episode 71250: reward: -13.629606450554187, Avg: -15.358738910454917, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.528, mean move frd: -0.07026592041827531\n",
      "\n",
      "Episode 71500: reward: -16.381959778949163, Avg: -16.403007745110934, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.444, mean move frd: -0.5919068697839995\n",
      "\n",
      "Episode 71750: reward: -19.287006675393748, Avg: -16.900282236930334, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.632, mean move frd: -0.09364716725784268\n",
      "\n",
      "Episode 72000: reward: -22.631580887591284, Avg: -21.10309746333195, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.8079999999999999, mean move frd: 0.23622163564038412\n",
      "\n",
      "Episode 72250: reward: -19.463829689650183, Avg: -16.58232316110835, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.528, mean move frd: 0.12152992352071808\n",
      "\n",
      "Episode 72500: reward: -12.987269186308419, Avg: -18.462428570469775, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.648, mean move frd: -0.2617098374361714\n",
      "\n",
      "Episode 72750: reward: -19.749759172036143, Avg: -18.070694401462543, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.76, mean move frd: -0.6426815572915116\n",
      "\n",
      "Episode 73000: reward: -12.466850613430413, Avg: -14.670980556479364, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.456, mean move frd: 0.12240188327671694\n",
      "\n",
      "Episode 73250: reward: -21.339693669065923, Avg: -16.701955437937478, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.652, mean move frd: -0.07674306235267521\n",
      "\n",
      "Episode 73500: reward: -20.368234511828476, Avg: -15.87262691322889, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.716, mean move frd: -0.13031494445792866\n",
      "\n",
      "Episode 73750: reward: -15.83869429822235, Avg: -14.380815931332041, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.68, mean move frd: -0.5705641070744003\n",
      "\n",
      "Episode 74000: reward: -5.753028244320829, Avg: -13.963493544716258, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.74, mean move frd: 1.2329949731520369\n",
      "\n",
      "Episode 74250: reward: -6.305079592275556, Avg: -13.02048383603927, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.46, mean move frd: 0.7499264824053232\n",
      "\n",
      "Episode 74500: reward: -8.344897976845864, Avg: -13.667637146238647, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.604, mean move frd: 0.9150661003947647\n",
      "\n",
      "Episode 74750: reward: -16.525755071137876, Avg: -14.401935634893482, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.48, mean move frd: 0.26092069881537505\n",
      "\n",
      "Episode 75000: reward: -7.5179625914632755, Avg: -13.396366886168408, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.56, mean move frd: -0.10299264919530601\n",
      "\n",
      "Episode 75250: reward: -41.97186682262198, Avg: -17.264605368416056, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.716, mean move frd: 0.04420652990844616\n",
      "\n",
      "Episode 75500: reward: -21.806172419280607, Avg: -14.93354615882077, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.46799999999999997, mean move frd: -0.2912267619173436\n",
      "\n",
      "Episode 75750: reward: -8.42482433042835, Avg: -14.115475510556587, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.456, mean move frd: 0.9199318758006886\n",
      "\n",
      "Episode 76000: reward: 6.673221714414936, Avg: -16.706818787977436, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.716, mean move frd: 0.29414950683055247\n",
      "\n",
      "Episode 76250: reward: -8.366471930983668, Avg: -14.170649572137169, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.524, mean move frd: 1.0693279360517312\n",
      "\n",
      "Episode 76500: reward: -10.31882126775304, Avg: -14.563885964553682, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.49200000000000005, mean move frd: 0.6925265908217365\n",
      "\n",
      "Episode 76750: reward: -0.2513584747635438, Avg: -14.807178738061353, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.684, mean move frd: 1.9769824764775497\n",
      "\n",
      "Episode 77000: reward: -17.794844313367307, Avg: -14.932326763520512, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.5760000000000001, mean move frd: -0.05932783936135859\n",
      "\n",
      "Episode 77250: reward: -14.585465610507757, Avg: -13.864714795551453, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.42, mean move frd: 0.009679163649995815\n",
      "\n",
      "Episode 77500: reward: -18.512898453633063, Avg: -14.624563458010622, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.456, mean move frd: -0.47895181502902595\n",
      "\n",
      "Episode 77750: reward: -6.178210302784081, Avg: -13.95128510644162, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.47600000000000003, mean move frd: 0.03917376555730403\n",
      "\n",
      "Episode 78000: reward: -9.523125039575751, Avg: -12.137523208174873, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.42, mean move frd: 0.2586552077469446\n",
      "\n",
      "Episode 78250: reward: -12.363939790597536, Avg: -12.622911641189548, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.424, mean move frd: -0.16792960549928218\n",
      "\n",
      "Episode 78500: reward: -8.99519055051365, Avg: -10.797889343765592, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.46399999999999997, mean move frd: 0.657086736212615\n",
      "\n",
      "Episode 78750: reward: -31.110963890497096, Avg: -12.683139961739348, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.544, mean move frd: 0.12419113150854955\n",
      "\n",
      "Episode 79000: reward: -13.087360938242405, Avg: -12.241936714578944, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.42, mean move frd: 0.03717614544956105\n",
      "\n",
      "Episode 79250: reward: -11.522741922227306, Avg: -9.629333773769124, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.37200000000000005, mean move frd: 1.4536708436133727\n",
      "\n",
      "Episode 79500: reward: -11.476627641640933, Avg: -12.39869116043531, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.528, mean move frd: -0.0690307719149505\n",
      "\n",
      "Episode 79750: reward: -15.877087800217318, Avg: -13.730317924590475, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.636, mean move frd: -0.1437604916917806\n",
      "\n",
      "Episode 80000: reward: -11.670975695944744, Avg: -12.718824076701143, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.508, mean move frd: 0.3262650649582768\n",
      "\n",
      "Episode 80250: reward: -11.920591881450012, Avg: -12.696041360822992, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.556, mean move frd: 0.016309520147098067\n",
      "\n",
      "Episode 80500: reward: -8.67142912920222, Avg: -13.70794496048057, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.68, mean move frd: -0.07104969699274673\n",
      "\n",
      "Episode 80750: reward: -15.296340974108013, Avg: -12.516350756525094, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.544, mean move frd: -0.2062320949226228\n",
      "\n",
      "Episode 81000: reward: -13.045781622964492, Avg: -11.345371172439519, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.5720000000000001, mean move frd: -0.20129119525799996\n",
      "\n",
      "Episode 81250: reward: -13.722577486702468, Avg: -12.509994400749937, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.66, mean move frd: -0.3254892638616019\n",
      "\n",
      "Episode 81500: reward: -12.125787424704914, Avg: -13.217603320018222, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.488, mean move frd: -0.41007920655054536\n",
      "\n",
      "Episode 81750: reward: -11.659799093574156, Avg: -11.745124819404866, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.608, mean move frd: 0.5392692681258053\n",
      "\n",
      "Episode 82000: reward: -21.781157010101975, Avg: -13.39987638608045, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.708, mean move frd: 0.31732314760461955\n",
      "\n",
      "Episode 82250: reward: -17.89860136840464, Avg: -12.717708408100034, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.64, mean move frd: 0.12764364065960673\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 82500: reward: -16.04435982489307, Avg: -13.96738810375162, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.552, mean move frd: -1.632953260249573\n",
      "\n",
      "Episode 82750: reward: -20.30685645842291, Avg: -15.067746298023696, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.588, mean move frd: -1.200678206033152\n",
      "\n",
      "Episode 83000: reward: -10.873901459690984, Avg: -12.909697080395617, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.484, mean move frd: -0.19663470481923384\n",
      "\n",
      "Episode 83250: reward: -9.618881374151327, Avg: -11.874964598759874, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.5760000000000001, mean move frd: -0.18676613465053205\n",
      "\n",
      "Episode 83500: reward: -11.153219736662738, Avg: -12.506078434690945, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.532, mean move frd: 0.7255858409421497\n",
      "\n",
      "Episode 83750: reward: -12.104514449633864, Avg: -11.524101394263848, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.508, mean move frd: 0.7745016025012156\n",
      "\n",
      "Episode 84000: reward: -34.64874144860149, Avg: -17.896356956559096, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.912, mean move frd: -1.5988843878660761\n",
      "\n",
      "Episode 84250: reward: -5.412078144013359, Avg: -11.751467020670553, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.56, mean move frd: 0.8505725757474923\n",
      "\n",
      "Episode 84500: reward: -9.52021739848808, Avg: -12.596065909445079, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.6759999999999999, mean move frd: 0.3142587855962415\n",
      "\n",
      "Episode 84750: reward: -15.953357308111269, Avg: -12.022699170381092, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.628, mean move frd: -0.03463473688687015\n",
      "\n",
      "Episode 85000: reward: -17.4783020242835, Avg: -10.947777674029567, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.512, mean move frd: -0.19716780154621655\n",
      "\n",
      "Episode 85250: reward: -13.613222452013023, Avg: -11.609596217545816, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.524, mean move frd: -0.5322998786794155\n",
      "\n",
      "Episode 85500: reward: -31.161544741270916, Avg: -10.713765464637344, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.596, mean move frd: 0.025971613943530515\n",
      "\n",
      "Episode 85750: reward: -15.223675714578947, Avg: -12.296365201440938, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.664, mean move frd: -0.22047670293163332\n",
      "\n",
      "Episode 86000: reward: -11.834588166117936, Avg: -11.621487468921602, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.5, mean move frd: 0.02651995548191516\n",
      "\n",
      "Episode 86250: reward: -10.261265579977975, Avg: -13.054719900364987, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.568, mean move frd: 0.1118578872977539\n",
      "\n",
      "Episode 86500: reward: -21.46825605406186, Avg: -14.576086460797963, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.6, mean move frd: 0.19164981767476813\n",
      "\n",
      "Episode 86750: reward: -19.595633449184128, Avg: -16.927045648510024, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.652, mean move frd: -0.05993684224248903\n",
      "\n",
      "Episode 87000: reward: -16.299600547556977, Avg: -13.51074744770996, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.524, mean move frd: -0.13407655573339167\n",
      "\n",
      "Episode 87250: reward: -23.63085375712823, Avg: -18.461574003939184, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.772, mean move frd: -0.7822964668771345\n",
      "\n",
      "Episode 87500: reward: -10.448238359785178, Avg: -13.084352992143781, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.508, mean move frd: 0.09071104738675455\n",
      "\n",
      "Episode 87750: reward: -9.738198629760161, Avg: -11.602044930214568, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.52, mean move frd: -0.04029767987431256\n",
      "\n",
      "Episode 88000: reward: -6.69316667324543, Avg: -12.514835988484558, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.7040000000000001, mean move frd: 1.0173169445502697\n",
      "\n",
      "Episode 88250: reward: -6.678574726770657, Avg: -11.891851145872874, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.516, mean move frd: 0.20024990350801325\n",
      "\n",
      "Episode 88500: reward: -13.589779958883522, Avg: -11.409531282039897, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.584, mean move frd: 0.07159072348056499\n",
      "\n",
      "Episode 88750: reward: -11.044359087301002, Avg: -12.279514655112514, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.688, mean move frd: -0.11358026940889672\n",
      "\n",
      "Episode 89000: reward: -10.622468058086223, Avg: -11.127136589693425, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.564, mean move frd: 0.21614011091920765\n",
      "\n",
      "Episode 89250: reward: -13.116513608255936, Avg: -12.25395657742402, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.56, mean move frd: 0.10342311457116327\n",
      "\n",
      "Episode 89500: reward: -17.343061195044626, Avg: -12.462666833323375, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.528, mean move frd: -0.19851979708324227\n",
      "\n",
      "Episode 89750: reward: -12.038136932164035, Avg: -13.470789874981445, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.644, mean move frd: -0.1329776034956336\n",
      "\n",
      "Episode 90000: reward: -10.142924470233677, Avg: -11.789408597913704, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.568, mean move frd: -0.20677340169227593\n",
      "\n",
      "Episode 90250: reward: -3.094606747226557, Avg: -11.184233185235168, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.516, mean move frd: 0.025603338592742463\n",
      "\n",
      "Episode 90500: reward: -10.587017591694806, Avg: -11.724726445929655, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.368, mean move frd: -1.1298758065927577\n",
      "\n",
      "Episode 90750: reward: -9.657063584698196, Avg: -9.42879214533316, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.37200000000000005, mean move frd: 0.8121679051517263\n",
      "\n",
      "Episode 91000: reward: -10.721877936634499, Avg: -10.902877187921703, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.336, mean move frd: -0.015918085103275574\n",
      "\n",
      "Episode 91250: reward: -11.100795585668141, Avg: -11.136439871736247, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.272, mean move frd: -0.03492287191003416\n",
      "\n",
      "Episode 91500: reward: -9.787904281209764, Avg: -10.961793990446704, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.4, mean move frd: 0.06643305211250186\n",
      "\n",
      "Episode 91750: reward: -12.402374336023788, Avg: -9.506417138507206, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.4, mean move frd: 0.6324914264281858\n",
      "\n",
      "Episode 92000: reward: -11.541589291229565, Avg: -11.236627305101583, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.5, mean move frd: 0.07749660226934363\n",
      "\n",
      "Episode 92250: reward: -13.340692727883873, Avg: -11.907759397209249, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.556, mean move frd: -0.6027516594104452\n",
      "\n",
      "Episode 92500: reward: -10.286175262797418, Avg: -9.800631606124316, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.54, mean move frd: -0.0800793736679543\n",
      "\n",
      "Episode 92750: reward: -4.69514544897438, Avg: -9.7189634069558, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.512, mean move frd: 0.5454026362960794\n",
      "\n",
      "Episode 93000: reward: -1.5865449898040733, Avg: -9.78758754532306, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.552, mean move frd: 0.7051945815483422\n",
      "\n",
      "Episode 93250: reward: -6.934031474717942, Avg: -9.894419342707234, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.6, mean move frd: -0.5089021720820697\n",
      "\n",
      "Episode 93500: reward: -7.066311477326621, Avg: -9.407164625855351, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.608, mean move frd: 0.262582656599058\n",
      "\n",
      "Episode 93750: reward: -13.51451580078671, Avg: -10.391849091141626, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.668, mean move frd: -0.0042190271540169634\n",
      "\n",
      "Episode 94000: reward: -8.450284595716699, Avg: -9.509020485919747, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.66, mean move frd: -0.23331686481207256\n",
      "\n",
      "Episode 94250: reward: -0.3280690863297959, Avg: -9.157188851667998, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.892, mean move frd: 0.6650561828023843\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 94500: reward: -8.578744925632524, Avg: -10.019116941268496, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.8520000000000001, mean move frd: -0.4745937367086201\n",
      "\n",
      "Episode 94750: reward: -7.373927972382921, Avg: -10.04147200702536, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.76, mean move frd: 0.23816860908724458\n",
      "\n",
      "Episode 95000: reward: -13.968258289188515, Avg: -15.14986856366867, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.788, mean move frd: -0.6589997940883952\n",
      "\n",
      "Episode 95250: reward: -3.7050886278254316, Avg: -9.872450226015602, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.664, mean move frd: 0.5705018523925095\n",
      "\n",
      "Episode 95500: reward: -7.173305087581601, Avg: -7.94898530611259, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.616, mean move frd: 1.2973024359486236\n",
      "\n",
      "Episode 95750: reward: -18.48815897404592, Avg: -11.038569495178539, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.54, mean move frd: -0.3881328087353061\n",
      "\n",
      "Episode 96000: reward: -5.141543831168545, Avg: -7.858367969127512, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.9279999999999999, mean move frd: -0.15937162939182237\n",
      "\n",
      "Episode 96250: reward: -7.111903232506481, Avg: -8.46968016524141, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.716, mean move frd: -0.7919850396318837\n",
      "\n",
      "Episode 96500: reward: -5.354862614584348, Avg: -6.764695065640301, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.6759999999999999, mean move frd: 0.17332745423991786\n",
      "\n",
      "Episode 96750: reward: -8.886424916678838, Avg: -7.949734310382131, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.636, mean move frd: 0.3302427720994993\n",
      "\n",
      "Episode 97000: reward: -9.500939700683105, Avg: -8.024586023831187, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.68, mean move frd: -0.09030544955286386\n",
      "\n",
      "Episode 97250: reward: -11.517275367794326, Avg: -10.090631651021814, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.7040000000000001, mean move frd: -0.18687575838998682\n",
      "\n",
      "Episode 97500: reward: -9.725651501921895, Avg: -9.851405084738193, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.7000000000000001, mean move frd: -0.23156367143155795\n",
      "\n",
      "Episode 97750: reward: -4.668696525660011, Avg: -8.483596658394038, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.684, mean move frd: 0.024401942330562764\n",
      "\n",
      "Episode 98000: reward: -3.8609194263789757, Avg: -7.62052617008916, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.708, mean move frd: 0.07974240282799597\n",
      "\n",
      "Episode 98250: reward: -11.768939130988786, Avg: -8.974763104433848, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.684, mean move frd: 0.05007431132728093\n",
      "\n",
      "Episode 98500: reward: -14.145114766095611, Avg: -8.513436628285456, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.6759999999999999, mean move frd: -0.31493242325998133\n",
      "\n",
      "Episode 98750: reward: -11.364054420866056, Avg: -9.716096332610487, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.652, mean move frd: -0.6422577904446961\n",
      "\n",
      "Episode 99000: reward: -6.507505430104244, Avg: -8.463387929326144, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.652, mean move frd: 0.16437165373924845\n",
      "\n",
      "Episode 99250: reward: -13.925582071826161, Avg: -9.209750540022274, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.6, mean move frd: -0.3086599534417522\n",
      "\n",
      "Episode 99500: reward: -1.6720757489422837, Avg: -7.649965980318763, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.684, mean move frd: 0.10411819240677112\n",
      "\n",
      "Episode 99750: reward: 2.8528012221661427, Avg: -9.048331523761371, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.828, mean move frd: 0.33872288408941\n",
      "\n",
      "Episode 100000: reward: -7.695172271600654, Avg: -5.9169509384148835, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.736, mean move frd: 0.5042786174954895\n",
      "\n",
      "Episode 100250: reward: -10.493195127705526, Avg: -6.404229916388499, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.6920000000000001, mean move frd: 0.070391353610224\n",
      "\n",
      "Episode 100500: reward: -2.2376798887768548, Avg: -6.169348907611845, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.7320000000000001, mean move frd: -0.24551912750944233\n",
      "\n",
      "Episode 100750: reward: -8.261625469458142, Avg: -6.652487668215899, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.784, mean move frd: -0.3190831700859831\n",
      "\n",
      "Episode 101000: reward: -7.8855399339101755, Avg: -6.550792640631474, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.78, mean move frd: -0.4815951845148745\n",
      "\n",
      "Episode 101250: reward: -6.490434636223504, Avg: -5.711304748655737, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.8440000000000001, mean move frd: 0.059450165809661816\n",
      "\n",
      "Episode 101500: reward: -3.6646368625788446, Avg: -4.855089640465502, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.8320000000000001, mean move frd: 0.40637680342023136\n",
      "\n",
      "Episode 101750: reward: -7.703523953401501, Avg: -5.379271432238005, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.648, mean move frd: 0.15311292664427584\n",
      "\n",
      "Episode 102000: reward: -3.8890143295961526, Avg: -5.639771578296485, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.7040000000000001, mean move frd: 0.16784500342567582\n",
      "\n",
      "Episode 102250: reward: -5.641698974803602, Avg: -5.010668758475182, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.7959999999999999, mean move frd: 0.9003213650422003\n",
      "\n",
      "Episode 102500: reward: -6.514278056548024, Avg: -5.95663740511174, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.668, mean move frd: -0.07434572698625765\n",
      "\n",
      "Episode 102750: reward: -4.435268233207024, Avg: -7.358610602374507, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.792, mean move frd: -0.3399549908146381\n",
      "\n",
      "Episode 103000: reward: -14.257713255632455, Avg: -7.728020461539598, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.812, mean move frd: -1.0991708527100592\n",
      "\n",
      "Episode 103250: reward: 2.605344239615505, Avg: -5.651138382416815, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.8440000000000001, mean move frd: 0.5867318513298445\n",
      "\n",
      "Episode 103500: reward: -9.600962041426467, Avg: -9.832158609633368, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.40399999999999997, mean move frd: 0.0153178873613397\n",
      "\n",
      "Episode 103750: reward: -12.016523197922645, Avg: -12.540673543305454, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.316, mean move frd: -0.14264548344633413\n",
      "\n",
      "Episode 104000: reward: -11.337935685565162, Avg: -9.870876661066204, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.652, mean move frd: 0.32506833983230743\n",
      "\n",
      "Episode 104250: reward: -7.889556599606784, Avg: -9.94847640155286, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.504, mean move frd: 0.38004756882895674\n",
      "\n",
      "Episode 104500: reward: -10.380959722012069, Avg: -8.301538258108625, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.584, mean move frd: 0.1201785864259696\n",
      "\n",
      "Episode 104750: reward: -8.156188525137962, Avg: -7.008065707737323, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.768, mean move frd: 0.3999152931094481\n",
      "\n",
      "Episode 105000: reward: -17.46249561648501, Avg: -16.33237577701007, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.004, mean move frd: -0.10370647688582135\n",
      "\n",
      "Episode 105250: reward: -11.449023866973679, Avg: -9.01307586398996, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.7120000000000001, mean move frd: -0.5019884383971265\n",
      "\n",
      "Episode 105500: reward: -4.722380542268171, Avg: -7.369608103564258, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.924, mean move frd: 0.6123068569603474\n",
      "\n",
      "Episode 105750: reward: 1.158358718441301, Avg: -6.261277628379337, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.628, mean move frd: 0.5678015848038795\n",
      "\n",
      "Episode 106000: reward: -2.6237028769622492, Avg: -4.615794538440566, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.64, mean move frd: 0.6128278103631299\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 106250: reward: -7.706275673578079, Avg: -9.850717611222318, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.8200000000000001, mean move frd: -0.3140099431347361\n",
      "\n",
      "Episode 106500: reward: -14.930789513726047, Avg: -6.272091607657351, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.748, mean move frd: -0.25528138461845673\n",
      "\n",
      "Episode 106750: reward: 2.2736051471073626, Avg: -7.686482291857274, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.8, mean move frd: -0.24467956411140185\n",
      "\n",
      "Episode 107000: reward: 4.447170656430238, Avg: -8.567278342400533, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.616, mean move frd: 0.06947535993407143\n",
      "\n",
      "Episode 107250: reward: 8.386040901906117, Avg: -12.059167390204006, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.49200000000000005, mean move frd: 0.3358709551499297\n",
      "\n",
      "Episode 107500: reward: -12.464381390328441, Avg: -11.438925668164481, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.768, mean move frd: 0.008678818774077834\n",
      "\n",
      "Episode 107750: reward: -7.316680053565904, Avg: -9.946764106274694, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.616, mean move frd: -0.80909496402787\n",
      "\n",
      "Episode 108000: reward: -10.178160391269385, Avg: -9.563525991863788, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.8, mean move frd: 0.05698165651223164\n",
      "\n",
      "Episode 108250: reward: -18.839475379207414, Avg: -6.529500363597364, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.9720000000000001, mean move frd: 0.34498899045364995\n",
      "\n",
      "Episode 108500: reward: -9.009715572834855, Avg: -9.912640489589275, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.788, mean move frd: -0.11479382582747381\n",
      "\n",
      "Episode 108750: reward: -15.939047401327311, Avg: -7.934399291624132, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.6759999999999999, mean move frd: -0.46492228709995453\n",
      "\n",
      "Episode 109000: reward: 0.08179464249417734, Avg: -6.617105005844349, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.696, mean move frd: -0.23923499410693\n",
      "\n",
      "Episode 109250: reward: -5.264412184182228, Avg: -3.783354426069063, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.8320000000000001, mean move frd: 0.22633903436423147\n",
      "\n",
      "Episode 109500: reward: -7.128973585278667, Avg: -5.691155870923677, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.652, mean move frd: 0.048831295497487395\n",
      "\n",
      "Episode 109750: reward: -1.036019451064064, Avg: -4.335819976192307, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.8240000000000001, mean move frd: 0.08323576670038549\n",
      "\n",
      "Episode 110000: reward: -7.249095624537659, Avg: -4.367015046709211, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.9359999999999999, mean move frd: -0.4905934136072834\n",
      "\n",
      "Episode 110250: reward: 5.897846569153273, Avg: -2.0648795444494565, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.92, mean move frd: 1.238023817731456\n",
      "\n",
      "Episode 110500: reward: -0.6390602891147079, Avg: -3.9073000586271314, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.8079999999999999, mean move frd: 0.051205530437778576\n",
      "\n",
      "Episode 110750: reward: -7.611041347737217, Avg: -3.7821645601913687, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.872, mean move frd: 0.046261875461115355\n",
      "\n",
      "Episode 111000: reward: -1.3258664561616893, Avg: -3.908951040042479, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.7320000000000001, mean move frd: 0.07041014907017074\n",
      "\n",
      "Episode 111250: reward: -5.328234834748297, Avg: -4.182163642353859, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.6920000000000001, mean move frd: 0.6084928206861399\n",
      "\n",
      "Episode 111500: reward: -3.26516662350197, Avg: -5.201413666136402, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.7000000000000001, mean move frd: -0.462552875652894\n",
      "\n",
      "Episode 111750: reward: -4.969956430772271, Avg: -3.259839574947084, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.9440000000000001, mean move frd: -0.03352957600789659\n",
      "\n",
      "Episode 112000: reward: -1.874249268202668, Avg: -3.791182821253863, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.8520000000000001, mean move frd: -0.8485250925378788\n",
      "\n",
      "Episode 112250: reward: -7.00062660066578, Avg: -4.830545028221802, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.68, mean move frd: -0.04425684104348211\n",
      "\n",
      "Episode 112500: reward: -6.082048298678956, Avg: -4.331110198488149, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.828, mean move frd: 0.060976813001832705\n",
      "\n",
      "Episode 112750: reward: -20.272893184701985, Avg: -6.298227252251008, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.98, mean move frd: -1.0776991104278877\n",
      "\n",
      "Episode 113000: reward: -10.3447438439126, Avg: -5.957361637757996, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.696, mean move frd: -0.11827107661074839\n",
      "\n",
      "Episode 113250: reward: -10.057124562077133, Avg: -7.653167847199741, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.62, mean move frd: 0.28588546019539474\n",
      "\n",
      "Episode 113500: reward: 0.15993823379990602, Avg: -5.420514857999206, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.84, mean move frd: -1.1404379982864683\n",
      "\n",
      "Episode 113750: reward: -16.57586500801723, Avg: -3.910994909371317, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.9520000000000001, mean move frd: -0.7270674875748385\n",
      "\n",
      "Episode 114000: reward: 1.7324855275271478, Avg: -0.25852406959310714, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.8959999999999999, mean move frd: -0.12575206891490567\n",
      "\n",
      "Episode 114250: reward: -4.82938875062167, Avg: -1.4823945082085477, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.06, mean move frd: -1.1850047554822196\n",
      "\n",
      "Episode 114500: reward: -17.049897970389594, Avg: -4.299798560168757, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.112, mean move frd: 0.6538472336552736\n",
      "\n",
      "Episode 114750: reward: -6.35952253046582, Avg: -2.6007704442556534, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.9359999999999999, mean move frd: -0.9007635231955502\n",
      "\n",
      "Episode 115000: reward: -16.829505542519186, Avg: -5.533423871246714, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.668, mean move frd: 0.07637588533627611\n",
      "\n",
      "Episode 115250: reward: -1.449011999722428, Avg: -5.761675255574792, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.664, mean move frd: 0.08497598481832451\n",
      "\n",
      "Episode 115500: reward: -2.667642599744582, Avg: -6.288951275469024, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.024, mean move frd: -0.07967682276949013\n",
      "\n",
      "Episode 115750: reward: 8.569508388591718, Avg: -2.236219107510797, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.7320000000000001, mean move frd: 1.7092013752786996\n",
      "\n",
      "Episode 116000: reward: 6.976853448903924, Avg: -1.8663495411304118, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.8640000000000001, mean move frd: 0.7849745078942441\n",
      "\n",
      "Episode 116250: reward: -4.164318364964892, Avg: -5.213609459150768, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.688, mean move frd: 0.04986208568847399\n",
      "\n",
      "Episode 116500: reward: 1.313811411744858, Avg: -1.6555019445037626, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.948, mean move frd: 0.3114864422122305\n",
      "\n",
      "Episode 116750: reward: 3.95691502945726, Avg: -1.0448162598751827, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.9440000000000001, mean move frd: 0.9248765617388315\n",
      "\n",
      "Episode 117000: reward: 1.9850168414873544, Avg: -2.6093622861378054, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.0, mean move frd: 0.36064278685456247\n",
      "\n",
      "Episode 117250: reward: 4.155427288776211, Avg: -2.259513464818327, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.064, mean move frd: 0.6087653831390616\n",
      "\n",
      "Episode 117500: reward: 3.558008806319835, Avg: -1.1175458776712213, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.2040000000000002, mean move frd: 1.3931331248951957\n",
      "\n",
      "Episode 117750: reward: -7.575576830314018, Avg: -5.395668990668154, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.128, mean move frd: -1.1052178318909895\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 118000: reward: -2.9134664030742092, Avg: -4.059782421732161, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.076, mean move frd: -0.9984031420380015\n",
      "\n",
      "Episode 118250: reward: -1.480483879051862, Avg: -4.479186474531893, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.032, mean move frd: 0.7113578332515855\n",
      "\n",
      "Episode 118500: reward: -2.3735565183178036, Avg: -1.889392013382227, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.872, mean move frd: -0.8515053028922746\n",
      "\n",
      "Episode 118750: reward: 1.015797415160204, Avg: -2.7591130316207457, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.7320000000000001, mean move frd: 0.17477743542148608\n",
      "\n",
      "Episode 119000: reward: 1.3289972851694873, Avg: -2.476923239795146, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.856, mean move frd: 0.23609512837294466\n",
      "\n",
      "Episode 119250: reward: 5.71588764973815, Avg: -2.629352493486241, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.848, mean move frd: 0.7702910704059607\n",
      "\n",
      "Episode 119500: reward: -12.690554730012614, Avg: -3.4118635801575374, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.08, mean move frd: 1.0848786227204612\n",
      "\n",
      "Episode 119750: reward: -10.227725337510885, Avg: -1.7592938395314026, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.128, mean move frd: -0.09054321137999502\n",
      "\n",
      "Episode 120000: reward: -3.6326603234682313, Avg: -4.38827520574976, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.752, mean move frd: 0.7464538407548608\n",
      "\n",
      "Episode 120250: reward: -7.477733980132809, Avg: -5.41732237278295, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.7040000000000001, mean move frd: 0.2689387976813358\n",
      "\n",
      "Episode 120500: reward: 17.384550948024653, Avg: -2.829968708044666, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.9520000000000001, mean move frd: 0.915644187198063\n",
      "\n",
      "Episode 120750: reward: -24.390079220919617, Avg: -6.315862487381155, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.8520000000000001, mean move frd: -0.5039040179880752\n",
      "\n",
      "Episode 121000: reward: -2.6725336501839907, Avg: -4.068347500194119, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.612, mean move frd: 0.2515803352238832\n",
      "\n",
      "Episode 121250: reward: -1.7944775408120748, Avg: -5.035973045653987, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.696, mean move frd: 0.2276724297613087\n",
      "\n",
      "Episode 121500: reward: 12.18912527214408, Avg: 0.2087582161429024, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.912, mean move frd: 1.0095755004695264\n",
      "\n",
      "Episode 121750: reward: 9.617552379223465, Avg: -1.782355756427264, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.236, mean move frd: 0.5101785567202348\n",
      "\n",
      "Episode 122000: reward: -4.79896762834632, Avg: -2.760079615333212, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.068, mean move frd: -1.1752752244061757\n",
      "\n",
      "Episode 122250: reward: -4.972177376551375, Avg: -5.379195950407644, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.288, mean move frd: -1.3871556061511723\n",
      "\n",
      "Episode 122500: reward: -9.533150344397631, Avg: -1.3423734476924412, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.892, mean move frd: 1.0898958516418609\n",
      "\n",
      "Episode 122750: reward: -36.666022801654236, Avg: -2.6795772047383286, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.1840000000000002, mean move frd: -0.42310513981233344\n",
      "\n",
      "Episode 123000: reward: 1.576192064385248, Avg: -1.794857687081612, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.9279999999999999, mean move frd: 0.03484000087780181\n",
      "\n",
      "Episode 123250: reward: -14.629845678651836, Avg: -5.089627822107448, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.668, mean move frd: -0.9268252251598096\n",
      "\n",
      "Episode 123500: reward: -10.061020167241113, Avg: -8.029236256775407, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.436, mean move frd: -0.11890195709462766\n",
      "\n",
      "Episode 123750: reward: -17.36499544645367, Avg: -4.966244496547175, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.708, mean move frd: -0.7920170639222656\n",
      "\n",
      "Episode 124000: reward: 2.2421766347432808, Avg: -4.465688547215607, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.716, mean move frd: -0.06356054087960872\n",
      "\n",
      "Episode 124250: reward: -0.9487417737403021, Avg: -3.220782779367839, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.004, mean move frd: -0.05132130069596728\n",
      "\n",
      "Episode 124500: reward: -11.10286733849651, Avg: -4.533065940312853, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.696, mean move frd: -0.10472982885627191\n",
      "\n",
      "Episode 124750: reward: -4.5924782080914195, Avg: -2.576286661345912, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.788, mean move frd: -0.22128731053618136\n",
      "\n",
      "Episode 125000: reward: -5.569599964934051, Avg: -3.940844445778166, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.912, mean move frd: -0.4459741897195351\n",
      "\n",
      "Episode 125250: reward: 1.2692112741584136, Avg: -1.404402795558517, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.024, mean move frd: 0.12957098155797397\n",
      "\n",
      "Episode 125500: reward: -12.065252582535319, Avg: -5.549790141718594, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.932, mean move frd: -0.6782122107851194\n",
      "\n",
      "Episode 125750: reward: -2.5441933294258785, Avg: -4.273030981985177, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.7320000000000001, mean move frd: -0.39919628150011943\n",
      "\n",
      "Episode 126000: reward: 3.2405394314312943, Avg: -0.33855313072263354, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.032, mean move frd: 0.7969348528316667\n",
      "\n",
      "Episode 126250: reward: 1.7216394502210335, Avg: -0.5197442432751262, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.056, mean move frd: 1.0534829757312618\n",
      "\n",
      "Episode 126500: reward: -1.4134426557139115, Avg: -2.621662380479817, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.7120000000000001, mean move frd: -0.015531591129360311\n",
      "\n",
      "Episode 126750: reward: 8.835139167168721, Avg: -1.2226164221520106, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.876, mean move frd: 0.4222362104428677\n",
      "\n",
      "Episode 127000: reward: -3.3071737709306905, Avg: -8.196056051078854, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.252, mean move frd: -0.2767848596508859\n",
      "\n",
      "Episode 127250: reward: -0.9759659165875405, Avg: -6.011725934931765, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.976, mean move frd: 0.5378089155707656\n",
      "\n",
      "Episode 127500: reward: -10.050183881165342, Avg: -5.575722917712933, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.9520000000000001, mean move frd: 0.3091364796128374\n",
      "\n",
      "Episode 127750: reward: 21.23355340807249, Avg: -2.4066003716765145, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.64, mean move frd: 2.6329629256190055\n",
      "\n",
      "Episode 128000: reward: -1.2117530044469387, Avg: -6.356154790051507, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.788, mean move frd: 1.298719981978943\n",
      "\n",
      "Episode 128250: reward: -9.757165480495146, Avg: -3.8811112961370355, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.064, mean move frd: 1.0739291584364303\n",
      "\n",
      "Episode 128500: reward: -9.88384588951465, Avg: -7.161995572045012, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.124, mean move frd: -0.1776823320972934\n",
      "\n",
      "Episode 128750: reward: -8.569820277341357, Avg: -3.069373291632349, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.804, mean move frd: 1.617891367578336\n",
      "\n",
      "Episode 129000: reward: 12.29365104114266, Avg: -2.5168656311552686, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.8640000000000001, mean move frd: 1.3775711221717608\n",
      "\n",
      "Episode 129250: reward: 15.060792057827175, Avg: -6.060902491211179, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.78, mean move frd: 0.8461445789221314\n",
      "\n",
      "Episode 129500: reward: -4.723241704869095, Avg: -6.710187547633512, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.4, mean move frd: 0.05415532611814711\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 129750: reward: 3.994810767595766, Avg: -4.473855068626131, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.524, mean move frd: 0.19516906591357266\n",
      "\n",
      "Episode 130000: reward: 7.138688504167208, Avg: -5.636291752833058, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.552, mean move frd: 0.5428260065617323\n",
      "\n",
      "Episode 130250: reward: -7.167605220927335, Avg: -10.54401116707609, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.4000000000000001, mean move frd: 0.4575803224258146\n",
      "\n",
      "Episode 130500: reward: 8.451039151998707, Avg: -6.51971569200972, model improved 1 times in 250 episodes.\n",
      "Mean survival time: 1.2, mean move frd: -0.5177512994326221\n",
      "\n",
      "Episode 130750: reward: -15.52361148483727, Avg: -3.53210653968612, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.056, mean move frd: 1.1435518017924848\n",
      "\n",
      "Episode 131000: reward: -7.730786468895148, Avg: -5.463063634141827, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.648, mean move frd: -0.27203207541233343\n",
      "\n",
      "Episode 131250: reward: -13.245297031052427, Avg: -3.0974688322103385, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.956, mean move frd: 0.8561350769091837\n",
      "\n",
      "Episode 131500: reward: -0.05053349615954161, Avg: 0.5006244385288193, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.996, mean move frd: -0.8856258934055425\n",
      "\n",
      "Episode 131750: reward: 0.4850422695550076, Avg: 1.6959081940283447, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.036, mean move frd: 0.17897486775156785\n",
      "\n",
      "Episode 132000: reward: 11.732438040793093, Avg: 2.4395358871444417, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.924, mean move frd: 0.5993250479997281\n",
      "\n",
      "Episode 132250: reward: -8.470633526226633, Avg: 2.97955913103509, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.068, mean move frd: -0.5237554148894691\n",
      "\n",
      "Episode 132500: reward: 11.223589719109645, Avg: 2.6144840026845335, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.996, mean move frd: 0.3676423543988154\n",
      "\n",
      "Episode 132750: reward: -6.408790367493465, Avg: -0.06195466926522375, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.012, mean move frd: 0.01260559410081923\n",
      "\n",
      "Episode 133000: reward: -5.731269955885534, Avg: 2.900179644995959, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.176, mean move frd: -0.9501196546054629\n",
      "\n",
      "Episode 133250: reward: -2.8736663945329894, Avg: 1.6923991316127438, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.024, mean move frd: -0.2722193190268155\n",
      "\n",
      "Episode 133500: reward: -3.3367781493629796, Avg: 0.4900691930881839, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.904, mean move frd: 0.5792911271460216\n",
      "\n",
      "Episode 133750: reward: -8.03819507453682, Avg: -0.809551685102852, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.124, mean move frd: -0.654133405017612\n",
      "\n",
      "Episode 134000: reward: -11.809186002207388, Avg: -2.2515449901064697, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.8200000000000001, mean move frd: -2.058104069500388\n",
      "\n",
      "Episode 134250: reward: -18.88642974384308, Avg: -1.1153218887834835, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.072, mean move frd: -0.34727724026235796\n",
      "\n",
      "Episode 134500: reward: 12.940230626557556, Avg: 0.285149587450371, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.088, mean move frd: 1.1748033950518146\n",
      "\n",
      "Episode 134750: reward: 6.034395589914711, Avg: -0.00789792437745529, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.092, mean move frd: 0.11153454715470978\n",
      "\n",
      "Episode 135000: reward: 10.369637820458577, Avg: -0.2191558529156296, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.12, mean move frd: -0.369803080816091\n",
      "\n",
      "Episode 135250: reward: -13.161412494895067, Avg: -4.326607289444475, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.272, mean move frd: -0.6405175886688455\n",
      "\n",
      "Episode 135500: reward: -27.6229247830366, Avg: -3.1402438386865557, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.156, mean move frd: -0.836577000853314\n",
      "\n",
      "Episode 135750: reward: 6.501573771019455, Avg: -2.110244622544461, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.84, mean move frd: -0.8964119323182201\n",
      "\n",
      "Episode 136000: reward: -6.176795027577303, Avg: 1.1562892204906263, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.988, mean move frd: 1.535613602396794\n",
      "\n",
      "Episode 136250: reward: -1.0506735841511414, Avg: -2.479202243828116, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.9, mean move frd: -2.158315811391027\n",
      "\n",
      "Episode 136500: reward: -36.828238804417694, Avg: -0.3093837315103869, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.224, mean move frd: 0.7797570010183534\n",
      "\n",
      "Episode 136750: reward: -0.54415273546301, Avg: 1.0470488720031956, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.18, mean move frd: 0.9477476391410077\n",
      "\n",
      "Episode 137000: reward: -18.789067304082785, Avg: -2.623288630858459, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.228, mean move frd: -3.0042608195150287\n",
      "\n",
      "Episode 137250: reward: -5.260930482731763, Avg: -0.3198619675833408, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.088, mean move frd: -0.048803177005155705\n",
      "\n",
      "Episode 137500: reward: 0.3280954131288425, Avg: -4.4222809498651925, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.836, mean move frd: -1.9540256529677582\n",
      "\n",
      "Episode 137750: reward: 0.14381068138615127, Avg: -1.3759795582152157, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.9400000000000001, mean move frd: 0.530467148170034\n",
      "\n",
      "Episode 138000: reward: -2.758709206243129, Avg: -1.2848605056781066, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.868, mean move frd: 0.04318160936025128\n",
      "\n",
      "Episode 138250: reward: 13.727490517452317, Avg: -0.26498769821325363, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.084, mean move frd: -0.42220498323189515\n",
      "\n",
      "Episode 138500: reward: -12.039256420629062, Avg: -3.076255163499435, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.124, mean move frd: -2.016969155035544\n",
      "\n",
      "Episode 138750: reward: -11.113160402724036, Avg: -0.5787575012873802, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.956, mean move frd: -1.4497432701252406\n",
      "\n",
      "Episode 139000: reward: -5.738055200067969, Avg: 1.585303533185624, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.056, mean move frd: -0.006121471935894807\n",
      "\n",
      "Episode 139250: reward: -12.867528341992074, Avg: -0.258784935569604, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.236, mean move frd: -1.402370307661806\n",
      "\n",
      "Episode 139500: reward: -1.098414828195935, Avg: -0.016225150060729377, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.064, mean move frd: -1.20541004817375\n",
      "\n",
      "Episode 139750: reward: 0.46604104847360617, Avg: -0.4172898430395996, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.296, mean move frd: -0.4315265509937373\n",
      "\n",
      "Episode 140000: reward: -33.69091085185889, Avg: -2.5967234436513964, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.84, mean move frd: -0.6802428877892698\n",
      "\n",
      "Episode 140250: reward: -6.1060614552578025, Avg: -7.2131956401519535, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.484, mean move frd: 0.1506084773604382\n",
      "\n",
      "Episode 140500: reward: -6.247537114670676, Avg: -2.8462612452946616, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.756, mean move frd: 0.010480291926575135\n",
      "\n",
      "Episode 140750: reward: 1.4249916763174557, Avg: -2.754520445183315, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.108, mean move frd: -0.16316358591439686\n",
      "\n",
      "Episode 141000: reward: -18.252831828155365, Avg: -2.1399449345243355, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.04, mean move frd: -0.8834888372447605\n",
      "\n",
      "Episode 141250: reward: 0.9715535930515315, Avg: 3.3402900231969115, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.9920000000000001, mean move frd: 0.12907547468243355\n",
      "\n",
      "Episode 141500: reward: 1.6280444163008205, Avg: 4.63594406709249, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.9840000000000001, mean move frd: 0.1746755271304919\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 141750: reward: 2.670860479425782, Avg: 4.217231713742319, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.996, mean move frd: -0.3954882271956207\n",
      "\n",
      "Episode 142000: reward: -4.003779003751403, Avg: 3.583294323344721, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.044, mean move frd: -0.46214629039639465\n",
      "\n",
      "Episode 142250: reward: -5.00535630460433, Avg: 5.137729399762643, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.176, mean move frd: -0.6072484545804693\n",
      "\n",
      "Episode 142500: reward: -5.832936147944361, Avg: 5.763055451057313, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.232, mean move frd: -0.3249284998516414\n",
      "\n",
      "Episode 142750: reward: -22.9870183014494, Avg: 3.035468951641784, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.048, mean move frd: -0.38709555895985587\n",
      "\n",
      "Episode 143000: reward: 14.862633308802124, Avg: 9.648905090721376, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.2919999999999998, mean move frd: 1.4083242865946748\n",
      "\n",
      "Episode 143250: reward: 2.7657751981994974, Avg: 5.467613503708814, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.156, mean move frd: -0.4440629334707138\n",
      "\n",
      "Episode 143500: reward: 16.39804435329951, Avg: 8.724321251188037, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.396, mean move frd: -0.5146218663235526\n",
      "\n",
      "Episode 143750: reward: 6.399251106205421, Avg: 12.115631851724647, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.588, mean move frd: -0.023456054474080234\n",
      "\n",
      "Episode 144000: reward: 5.820829971692245, Avg: 4.009087464251739, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.4000000000000001, mean move frd: -1.4130728157886996\n",
      "\n",
      "Episode 144250: reward: 12.274394297983951, Avg: 6.46041593666362, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.4080000000000001, mean move frd: 0.7038182487414415\n",
      "\n",
      "Episode 144500: reward: 12.518744711511335, Avg: 7.474306659032363, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.32, mean move frd: 0.4404937195762898\n",
      "\n",
      "Episode 144750: reward: -0.7230620432118329, Avg: 11.0785275256868, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.392, mean move frd: -0.20957017829252997\n",
      "\n",
      "Episode 145000: reward: 28.402577826852585, Avg: 14.146557699116675, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.608, mean move frd: 3.0761052859652622\n",
      "\n",
      "Episode 145250: reward: 9.870669016163843, Avg: 12.783939902004539, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.54, mean move frd: -0.039584034223828016\n",
      "\n",
      "Episode 145500: reward: 8.06982936338751, Avg: 16.713975248411835, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.7280000000000002, mean move frd: -0.1830194762179337\n",
      "\n",
      "Episode 145750: reward: 7.226378347360347, Avg: 17.104677104650953, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.588, mean move frd: -0.2773505967953883\n",
      "\n",
      "Episode 146000: reward: 10.82738700063827, Avg: 111.02665810306348, model improved 3 times in 250 episodes.\n",
      "Mean survival time: 6.304, mean move frd: -0.039970410375625674\n",
      "\n",
      "Episode 146250: reward: 136.38813171285014, Avg: 129.0095266973499, model improved 3 times in 250 episodes.\n",
      "Mean survival time: 7.32, mean move frd: 0.013621462484598213\n",
      "\n",
      "Episode 146500: reward: -14.161620946243989, Avg: 200.3863785881129, model improved 1 times in 250 episodes.\n",
      "Mean survival time: 10.892000000000001, mean move frd: -0.22746349725823248\n",
      "\n",
      "Episode 146750: reward: -4.891698482452603, Avg: 41.69216677307745, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.072, mean move frd: -0.43633009040426957\n",
      "\n",
      "Episode 147000: reward: -12.40653463451348, Avg: -0.013600641737459184, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.276, mean move frd: -0.1075364129229384\n",
      "\n",
      "Episode 147250: reward: 377.7069225443801, Avg: 109.14095684437498, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 6.064, mean move frd: -0.11779895738419248\n",
      "\n",
      "Episode 147500: reward: 167.63592638516755, Avg: 69.93769001905528, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 4.248, mean move frd: 0.6886852766156905\n",
      "\n",
      "Episode 147750: reward: 414.89522400007314, Avg: 295.93462063781453, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 14.82, mean move frd: -0.42641072863163243\n",
      "\n",
      "Episode 148000: reward: 405.881885536459, Avg: 150.07698838436204, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 8.216000000000001, mean move frd: 0.8131155083261747\n",
      "\n",
      "Episode 148250: reward: 9.62954908962606, Avg: 40.30560489559336, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.8680000000000003, mean move frd: -0.8871246917277242\n",
      "\n",
      "Episode 148500: reward: 124.47295803037247, Avg: 86.50960174796403, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 4.96, mean move frd: 0.447769875458312\n",
      "\n",
      "Episode 148750: reward: 429.6654263158213, Avg: 195.90163077547442, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 9.852, mean move frd: 0.26869674960379314\n",
      "\n",
      "Episode 149000: reward: 399.57152824886026, Avg: 148.59734895696394, model improved 3 times in 250 episodes.\n",
      "Mean survival time: 7.667999999999999, mean move frd: 0.26919062943173494\n",
      "\n",
      "Episode 149250: reward: -2.096805129171136, Avg: -4.866052677554323, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.792, mean move frd: 0.6976818396957303\n",
      "\n",
      "Episode 149500: reward: 85.73582668066028, Avg: 4.089653660079169, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.244, mean move frd: 1.0500259748208485\n",
      "\n",
      "Episode 149750: reward: 29.426644443617548, Avg: 57.85303534990991, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.62, mean move frd: -0.08966651321265058\n",
      "\n",
      "Episode 150000: reward: 3.1866448447099405, Avg: 92.31002941059769, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 5.276000000000001, mean move frd: -0.20579022151468312\n",
      "\n",
      "Episode 150250: reward: 1.1024778397393735, Avg: 47.5951115102919, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.208, mean move frd: 0.07119762680397368\n",
      "\n",
      "Episode 150500: reward: 19.59458573598883, Avg: -1.146954345144237, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.7240000000000001, mean move frd: -0.21246006565075565\n",
      "\n",
      "Episode 150750: reward: -7.983247657259611, Avg: -1.4462115401054156, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.6920000000000001, mean move frd: -0.11039387878291643\n",
      "\n",
      "Episode 151000: reward: 27.503478102574796, Avg: 84.52371714915351, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 4.988, mean move frd: -0.3544873881785943\n",
      "\n",
      "Episode 151250: reward: 22.093025616578988, Avg: 42.07562014003944, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.8080000000000003, mean move frd: 0.4780574646388013\n",
      "\n",
      "Episode 151500: reward: 23.446438113464392, Avg: 76.4129531802434, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 4.54, mean move frd: 1.0931849460137522\n",
      "\n",
      "Episode 151750: reward: -9.03536045626721, Avg: 183.4708801502963, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 10.575999999999999, mean move frd: -0.054503238822183\n",
      "\n",
      "Episode 152000: reward: 2.112781314870512, Avg: 21.488873663282448, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.18, mean move frd: -0.4483249961602116\n",
      "\n",
      "Episode 152250: reward: 53.92983925750133, Avg: 32.82159615225246, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.872, mean move frd: 0.29089348312953056\n",
      "\n",
      "Episode 152500: reward: 63.924447149650284, Avg: 107.65274283913054, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 6.128, mean move frd: 0.06171942598778222\n",
      "\n",
      "Episode 152750: reward: 252.78125333613158, Avg: 67.85084831094602, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 5.024, mean move frd: 0.030852550839926173\n",
      "\n",
      "Episode 153000: reward: 2.7982825208793987, Avg: 28.44629389127179, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.3040000000000003, mean move frd: -0.5519453505195027\n",
      "\n",
      "Episode 153250: reward: 17.279223221079214, Avg: 53.267683203397624, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.72, mean move frd: -0.1365381910576417\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 153500: reward: -12.892106631863042, Avg: 46.02730572922333, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.3680000000000003, mean move frd: -1.3356527476238198\n",
      "\n",
      "Episode 153750: reward: 241.34390179236456, Avg: 82.35475341321401, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 5.0360000000000005, mean move frd: -0.12737450799632227\n",
      "\n",
      "Episode 154000: reward: 16.590170506044835, Avg: 115.03812468684478, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 6.664, mean move frd: -0.0018975531437066318\n",
      "\n",
      "Episode 154250: reward: -10.811619787846228, Avg: 51.94313300014521, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.12, mean move frd: -0.6513090636700237\n",
      "\n",
      "Episode 154500: reward: 31.68992275973407, Avg: 54.57235739962243, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.444, mean move frd: 0.22931663942931052\n",
      "\n",
      "Episode 154750: reward: -10.868962260752973, Avg: 4.609650434194553, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.4480000000000002, mean move frd: -0.4704633650691544\n",
      "\n",
      "Episode 155000: reward: -4.030446211333963, Avg: -5.4141205783835975, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.304, mean move frd: 0.3883887017948867\n",
      "\n",
      "Episode 155250: reward: 20.699660630414797, Avg: 23.33292640315339, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.128, mean move frd: -0.2824778116232626\n",
      "\n",
      "Episode 155500: reward: -9.299294406462803, Avg: 66.46631122822602, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 4.632, mean move frd: -0.05701987302522178\n",
      "\n",
      "Episode 155750: reward: -6.835400546114087, Avg: 3.63621627876376, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.136, mean move frd: -0.506044811953515\n",
      "\n",
      "Episode 156000: reward: -31.994291855882416, Avg: 50.72354669375596, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.484, mean move frd: -0.03417849561391648\n",
      "\n",
      "Episode 156250: reward: -5.3449477572259205, Avg: 0.3046414763453125, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.932, mean move frd: -0.7427993945513063\n",
      "\n",
      "Episode 156500: reward: -21.610777694596283, Avg: -5.5582925147175315, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.876, mean move frd: -0.5010348093436294\n",
      "\n",
      "Episode 156750: reward: 94.7751052929362, Avg: 114.4266965740558, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 7.004, mean move frd: 0.2684607661299404\n",
      "\n",
      "Episode 157000: reward: -8.42639833638771, Avg: 98.54226932189154, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 6.22, mean move frd: -1.3097296736633397\n",
      "\n",
      "Episode 157250: reward: 93.03557945164499, Avg: 30.636973265722883, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.64, mean move frd: 0.24325050031873854\n",
      "\n",
      "Episode 157500: reward: -8.235462590204191, Avg: -8.056669695654989, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.552, mean move frd: 0.04450843402327882\n",
      "\n",
      "Episode 157750: reward: 22.392552300055705, Avg: -0.4211169344275788, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.8840000000000001, mean move frd: -0.180418196466473\n",
      "\n",
      "Episode 158000: reward: -0.43626598319381316, Avg: -2.524104393081271, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.86, mean move frd: -0.49025937670438147\n",
      "\n",
      "Episode 158250: reward: -7.089232067361868, Avg: 3.4757869737240688, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.364, mean move frd: -0.12915633091801504\n",
      "\n",
      "Episode 158500: reward: 20.169061786954224, Avg: 8.73763636260521, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.3519999999999999, mean move frd: 1.0798470284963633\n",
      "\n",
      "Episode 158750: reward: 1.9737314066834823, Avg: -2.817378199063703, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.924, mean move frd: 0.01569955880289107\n",
      "\n",
      "Episode 159000: reward: -5.859679894248974, Avg: -5.374407660694393, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.6759999999999999, mean move frd: -0.34509444457508487\n",
      "\n",
      "Episode 159250: reward: -0.3955858423864189, Avg: -3.947316496019274, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.8200000000000001, mean move frd: 0.9681808244790264\n",
      "\n",
      "Episode 159500: reward: 19.473610195018622, Avg: 68.47901479336315, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 4.784, mean move frd: -0.6127651719951828\n",
      "\n",
      "Episode 159750: reward: 338.09109283579323, Avg: 39.503937891419405, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.0, mean move frd: 0.42884502674890107\n",
      "\n",
      "Episode 160000: reward: 51.892072565889485, Avg: 42.618561092192365, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.9560000000000004, mean move frd: -0.18670335341348557\n",
      "\n",
      "Episode 160250: reward: 4.8550195076432985, Avg: 6.3836195256945105, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.1640000000000001, mean move frd: -0.17237501206268074\n",
      "\n",
      "Episode 160500: reward: 100.10005962566233, Avg: 23.159384400428657, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.996, mean move frd: -0.06918899157823392\n",
      "\n",
      "Episode 160750: reward: 1.5923857624129898, Avg: 13.749982239183788, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.58, mean move frd: -0.23334947908937384\n",
      "\n",
      "Episode 161000: reward: 30.62173890846187, Avg: 29.081840871919503, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.2119999999999997, mean move frd: 0.12426831589965728\n",
      "\n",
      "Episode 161250: reward: 75.70766714806228, Avg: 100.96421222319084, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 5.848, mean move frd: -0.21781653136528764\n",
      "\n",
      "Episode 161500: reward: -7.147694403545604, Avg: 104.44958777622591, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 6.216, mean move frd: 0.012164354951537814\n",
      "\n",
      "Episode 161750: reward: 73.67628200732183, Avg: 57.244895658591915, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 4.08, mean move frd: -0.11673796024240128\n",
      "\n",
      "Episode 162000: reward: 12.419707207332802, Avg: 11.168997214442758, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.42, mean move frd: -0.27830107232109513\n",
      "\n",
      "Episode 162250: reward: 10.781327671378396, Avg: 31.17633726528309, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.464, mean move frd: 0.40781292292277127\n",
      "\n",
      "Episode 162500: reward: 12.91036794606118, Avg: 89.09011827213142, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 5.252000000000001, mean move frd: 0.14792151078819643\n",
      "\n",
      "Episode 162750: reward: 0.24697066311702365, Avg: 43.56440819272348, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.4360000000000004, mean move frd: -1.519147495962984\n",
      "\n",
      "Episode 163000: reward: 9.072677224144638, Avg: 114.18205182681717, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 6.124, mean move frd: 0.3784078508919527\n",
      "\n",
      "Episode 163250: reward: -1.6589603970566689, Avg: 75.46511835355987, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 4.712, mean move frd: -0.0027911830589898655\n",
      "\n",
      "Episode 163500: reward: 3.19892919719579, Avg: 14.204118469816894, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.6159999999999999, mean move frd: -0.18455793855031424\n",
      "\n",
      "Episode 163750: reward: -14.953285794724756, Avg: 13.116215562135201, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.6680000000000001, mean move frd: -0.09431002234830074\n",
      "\n",
      "Episode 164000: reward: 318.4406174431278, Avg: 73.44958889420941, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 5.0, mean move frd: 0.011813994234663384\n",
      "\n",
      "Episode 164250: reward: 387.0394122185022, Avg: 337.4792637414548, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 17.548, mean move frd: 0.11067875204539494\n",
      "\n",
      "Episode 164500: reward: 22.40582586065336, Avg: 83.92700588831747, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 5.224, mean move frd: 0.629195318935419\n",
      "\n",
      "Episode 164750: reward: 89.48280133349309, Avg: 49.804302066427475, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.252, mean move frd: 0.3708038902935554\n",
      "\n",
      "Episode 165000: reward: 19.881097828164577, Avg: 39.45959158026689, model improved 1 times in 250 episodes.\n",
      "Mean survival time: 2.676, mean move frd: -0.26797737951081624\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 165250: reward: 6.104334079488302, Avg: 83.84847949313745, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 4.792, mean move frd: 0.1525635944129892\n",
      "\n",
      "Episode 165500: reward: 65.26412438007489, Avg: 109.29570154344226, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 6.412000000000001, mean move frd: -0.5953405564136274\n",
      "\n",
      "Episode 165750: reward: 11.077129462323105, Avg: 38.82667624073113, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.72, mean move frd: 0.16193027097826213\n",
      "\n",
      "Episode 166000: reward: 2.1986594759920006, Avg: 128.90045409148507, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 7.344, mean move frd: 0.0257633717764425\n",
      "\n",
      "Episode 166250: reward: 402.388263341385, Avg: 99.55964691826487, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 5.78, mean move frd: -0.1502399964058231\n",
      "\n",
      "Episode 166500: reward: 6.526468014032341, Avg: 52.84806792707557, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.252, mean move frd: 0.2953855591332168\n",
      "\n",
      "Episode 166750: reward: 221.83802451466707, Avg: 83.62666232284406, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 4.64, mean move frd: 1.4843657678912185\n",
      "\n",
      "Episode 167000: reward: 405.1728557006502, Avg: 111.58120936864557, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 6.304, mean move frd: -0.14353827796026594\n",
      "\n",
      "Episode 167250: reward: 48.25633383080783, Avg: 36.96391622627975, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.504, mean move frd: -0.14812229851901254\n",
      "\n",
      "Episode 167500: reward: 17.420699590720364, Avg: 95.16918166155205, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 5.24, mean move frd: -0.6161121347957079\n",
      "\n",
      "Episode 167750: reward: 10.163509267070173, Avg: -6.6797811932474485, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.664, mean move frd: -0.6039663127207945\n",
      "\n",
      "Episode 168000: reward: -8.449564430211566, Avg: -1.9617533415286637, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.7320000000000001, mean move frd: -0.2008176157218927\n",
      "\n",
      "Episode 168250: reward: -5.930145691393128, Avg: 128.01449447421027, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 7.4479999999999995, mean move frd: -0.5470585478686854\n",
      "\n",
      "Episode 168500: reward: 0.8321353706898034, Avg: -0.8865345400205887, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.7759999999999999, mean move frd: 0.31474985688879376\n",
      "\n",
      "Episode 168750: reward: -26.391322385016878, Avg: 7.645281520466736, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.696, mean move frd: 0.47094274414861576\n",
      "\n",
      "Episode 169000: reward: -6.109431468803943, Avg: -4.315451838259383, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.7440000000000001, mean move frd: -0.057066515805940465\n",
      "\n",
      "Episode 169250: reward: 2.553573817187397, Avg: 14.222242586524697, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.568, mean move frd: 0.6287609953507504\n",
      "\n",
      "Episode 169500: reward: 3.360616479333167, Avg: 36.03671995908341, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.8080000000000003, mean move frd: -0.0871042514996824\n",
      "\n",
      "Episode 169750: reward: 85.5736211795616, Avg: 95.61583531942003, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 5.6160000000000005, mean move frd: 0.1399407264174603\n",
      "\n",
      "Episode 170000: reward: 1.2568378045132569, Avg: 50.6931879266909, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.8760000000000003, mean move frd: -0.44079379102776484\n",
      "\n",
      "Episode 170250: reward: 25.552607316791352, Avg: 120.55652396912151, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 7.0120000000000005, mean move frd: -0.046240046144507274\n",
      "\n",
      "Episode 170500: reward: 74.68352024335371, Avg: 178.27573442504843, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 9.072000000000001, mean move frd: -0.844985063976055\n",
      "\n",
      "Episode 170750: reward: 10.892917085518471, Avg: 120.9017206191281, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 6.496, mean move frd: 0.23044542747374588\n",
      "\n",
      "Episode 171000: reward: 198.02624745537665, Avg: 158.62607377133952, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 8.108, mean move frd: 0.5137156346830103\n",
      "\n",
      "Episode 171250: reward: 24.067929223176968, Avg: 41.28979924396019, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.9960000000000004, mean move frd: -0.11167126349609646\n",
      "\n",
      "Episode 171500: reward: 19.272288166194784, Avg: 18.789790074574945, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.716, mean move frd: -0.33062453661784125\n",
      "\n",
      "Episode 171750: reward: 23.31271410636934, Avg: 31.951434522846746, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.2880000000000003, mean move frd: 0.6061601899371539\n",
      "\n",
      "Episode 172000: reward: 138.58019421289654, Avg: 147.3828141247917, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 7.876, mean move frd: 0.009350439739312533\n",
      "\n",
      "Episode 172250: reward: 64.05474344448336, Avg: 57.07951917808665, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.764, mean move frd: 0.198388947615332\n",
      "\n",
      "Episode 172500: reward: 389.9175595626869, Avg: 297.56110852475405, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 15.655999999999999, mean move frd: -0.418376262350188\n",
      "\n",
      "Episode 172750: reward: 13.652270667718678, Avg: 1.4595145797382596, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.024, mean move frd: 0.1585698727847023\n",
      "\n",
      "Episode 173000: reward: -4.882390531508811, Avg: 42.800387712589306, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.028, mean move frd: 0.1894235191848218\n",
      "\n",
      "Episode 173250: reward: 10.081316115208637, Avg: 5.700175686226098, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.04, mean move frd: 0.32834864855807594\n",
      "\n",
      "Episode 173500: reward: 2.4208861230606544, Avg: 67.9083673235483, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 4.208, mean move frd: -0.15339390885568985\n",
      "\n",
      "Episode 173750: reward: 23.403907088533394, Avg: 88.83309182732968, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 5.4, mean move frd: 0.10346994806657035\n",
      "\n",
      "Episode 174000: reward: 28.602415627803534, Avg: 154.66557073233702, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 8.564, mean move frd: 0.42204817341828155\n",
      "\n",
      "Episode 174250: reward: -7.906921858664868, Avg: -0.8917196815892862, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.6840000000000002, mean move frd: -0.256191924682423\n",
      "\n",
      "Episode 174500: reward: 116.44290718870913, Avg: 129.01935662515388, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 7.588, mean move frd: 0.19246730581944518\n",
      "\n",
      "Episode 174750: reward: 139.3419373677548, Avg: 103.80336525610059, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 5.916, mean move frd: 0.20447020327808457\n",
      "\n",
      "Episode 175000: reward: 180.05121566387157, Avg: 42.431350641821254, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.3000000000000003, mean move frd: -0.3383898421854143\n",
      "\n",
      "Episode 175250: reward: -6.178075580125849, Avg: 13.386120147495195, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.5080000000000002, mean move frd: -0.08911673948746521\n",
      "\n",
      "Episode 175500: reward: 38.59600828789724, Avg: 60.92136326369897, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.9760000000000004, mean move frd: 0.07479273864861788\n",
      "\n",
      "Episode 175750: reward: 33.12920202966936, Avg: 16.48679090975256, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.636, mean move frd: 0.04892717628104124\n",
      "\n",
      "Episode 176000: reward: -11.989619926216628, Avg: 238.8350498012509, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 12.328, mean move frd: 0.1124483705694435\n",
      "\n",
      "Episode 176250: reward: 361.2261108354705, Avg: 183.67623966051823, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 10.088, mean move frd: 1.6081130714334169\n",
      "\n",
      "Episode 176500: reward: 406.68865785287113, Avg: 234.6792647035465, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 12.040000000000001, mean move frd: -0.11240059304547408\n",
      "\n",
      "Episode 176750: reward: 317.4458011862887, Avg: 199.77257020436346, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 10.475999999999999, mean move frd: 0.25689307722533794\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 177000: reward: 409.9109081355952, Avg: 247.2773327168573, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 13.024000000000001, mean move frd: -0.04886738377976459\n",
      "\n",
      "Episode 177250: reward: 428.19672394143026, Avg: 225.95118399833027, model improved 1 times in 250 episodes.\n",
      "Mean survival time: 11.168, mean move frd: 0.0909947172923848\n",
      "\n",
      "Episode 177500: reward: 400.43423516683083, Avg: 128.87174105955, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 7.084, mean move frd: 0.4737893562173278\n",
      "\n",
      "Episode 177750: reward: 5.067214895962508, Avg: 2.2056060549257985, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.168, mean move frd: 0.49238010106728136\n",
      "\n",
      "Episode 178000: reward: 9.888149055277506, Avg: 58.86323749589002, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.7119999999999997, mean move frd: -0.36228189116800225\n",
      "\n",
      "Episode 178250: reward: 33.13956460850538, Avg: 71.82742762140505, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 4.512, mean move frd: 0.11338211672776843\n",
      "\n",
      "Episode 178500: reward: 317.7993221450081, Avg: 111.86295660678695, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 6.247999999999999, mean move frd: -0.037444283140433174\n",
      "\n",
      "Episode 178750: reward: 13.989336633616968, Avg: 71.27373589791775, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 4.452, mean move frd: -0.3288230222046897\n",
      "\n",
      "Episode 179000: reward: 9.938714675400607, Avg: 99.00033185444344, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 5.54, mean move frd: -0.19347617263367162\n",
      "\n",
      "Episode 179250: reward: 12.695280803765334, Avg: 58.580995097422395, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.6239999999999997, mean move frd: -0.04315474876805947\n",
      "\n",
      "Episode 179500: reward: 16.324508688538206, Avg: 102.419173893374, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 5.624, mean move frd: -0.6794435377512925\n",
      "\n",
      "Episode 179750: reward: 202.1053273168336, Avg: 161.1131969908265, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 8.724, mean move frd: -0.2922751355575323\n",
      "\n",
      "Episode 180000: reward: 104.89738787632746, Avg: 317.8313677727363, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 15.915999999999999, mean move frd: 0.6130032443680038\n",
      "\n",
      "Episode 180250: reward: -2.9427671934909068, Avg: -0.8660986830967433, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.612, mean move frd: 0.3003576575345689\n",
      "\n",
      "Episode 180500: reward: 17.195748558862277, Avg: 14.775737083121935, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.06, mean move frd: 0.09853983221605338\n",
      "\n",
      "Episode 180750: reward: 62.499935382754764, Avg: 85.57475591897048, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 4.956, mean move frd: 1.9324249263529687\n",
      "\n",
      "Episode 181000: reward: 43.815985271911394, Avg: 46.52765393981886, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.952, mean move frd: 0.05956647771057013\n",
      "\n",
      "Episode 181250: reward: -0.74889559645068, Avg: 30.503870974733594, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.2680000000000002, mean move frd: -0.4328307490570039\n",
      "\n",
      "Episode 181500: reward: 78.6307787974924, Avg: 119.4456885461204, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 6.784, mean move frd: 0.6188052048783905\n",
      "\n",
      "Episode 181750: reward: -9.934389352723143, Avg: 127.1289790654602, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 6.88, mean move frd: 0.11038599053323317\n",
      "\n",
      "Episode 182000: reward: 3.319946171066956, Avg: 0.4648221827411902, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.72, mean move frd: 0.09313782000988971\n",
      "\n",
      "Episode 182250: reward: 22.53469967800983, Avg: 52.8834717735675, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.4, mean move frd: -0.12014889306911196\n",
      "\n",
      "Episode 182500: reward: 412.91934933363336, Avg: 157.6289487091811, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 8.267999999999999, mean move frd: 0.5614927278862686\n",
      "\n",
      "Episode 182750: reward: 13.48745258570566, Avg: 46.502973573199085, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.176, mean move frd: 0.6970490985418467\n",
      "\n",
      "Episode 183000: reward: -6.248378577210531, Avg: -5.627976425463342, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.504, mean move frd: 0.006518728101800786\n",
      "\n",
      "Episode 183250: reward: 70.00926629235738, Avg: 18.541434034896785, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.9080000000000001, mean move frd: 0.6542837167317728\n",
      "\n",
      "Episode 183500: reward: 345.47634834884195, Avg: 37.57678646728858, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.924, mean move frd: 0.5046782122324377\n",
      "\n",
      "Episode 183750: reward: 356.6748144552642, Avg: 133.16890984136077, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 7.732, mean move frd: -0.5544076071741275\n",
      "\n",
      "Episode 184000: reward: 61.46582848777906, Avg: 45.27585531405003, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.228, mean move frd: 0.9736806309072834\n",
      "\n",
      "Episode 184250: reward: 60.404589610654945, Avg: 81.61355812927475, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 5.084, mean move frd: 0.6494182608720817\n",
      "\n",
      "Episode 184500: reward: 7.200965790297332, Avg: -1.3178142654179772, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.0959999999999999, mean move frd: -1.4901665860231887\n",
      "\n",
      "Episode 184750: reward: 44.236440764142834, Avg: 235.22058496515388, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 11.572000000000001, mean move frd: 0.6546882228133396\n",
      "\n",
      "Episode 185000: reward: 20.880039557452815, Avg: 57.68688667447003, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.524, mean move frd: 0.2985679883962481\n",
      "\n",
      "Episode 185250: reward: 74.80822906001215, Avg: 195.1455882299287, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 9.884, mean move frd: 0.37413352076721634\n",
      "\n",
      "Episode 185500: reward: 14.649338018049445, Avg: 130.04641066535245, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 7.2, mean move frd: 0.7027152580725174\n",
      "\n",
      "Episode 185750: reward: 27.085060759822987, Avg: 80.91321560241163, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 5.2, mean move frd: 1.9137117470543732\n",
      "\n",
      "Episode 186000: reward: 148.39051991174432, Avg: 51.9474818132433, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.508, mean move frd: 1.3353362596313587\n",
      "\n",
      "Episode 186250: reward: 221.1416346498943, Avg: 209.093129332072, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 11.1, mean move frd: -1.3981990812531255\n",
      "\n",
      "Episode 186500: reward: 37.88089167123687, Avg: 98.98384468738608, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 5.9479999999999995, mean move frd: 0.6932653762357799\n",
      "\n",
      "Episode 186750: reward: 13.896249024258832, Avg: 251.91019885031997, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 13.328, mean move frd: 0.36728086771231677\n",
      "\n",
      "Episode 187000: reward: 26.644364588709813, Avg: 40.20189509046318, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.116, mean move frd: 0.08780769004342263\n",
      "\n",
      "Episode 187250: reward: 97.97632303273242, Avg: 61.497858192678585, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 4.28, mean move frd: 1.0443584298799287\n",
      "\n",
      "Episode 187500: reward: 58.190570707633995, Avg: 77.0684949092511, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 4.948, mean move frd: 0.12842665453092666\n",
      "\n",
      "Episode 187750: reward: 32.25459132604004, Avg: 97.4102628113421, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 5.48, mean move frd: 2.0557105018120327\n",
      "\n",
      "Episode 188000: reward: -39.42005624680438, Avg: -18.387480929061734, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.112, mean move frd: -0.27271502107922246\n",
      "\n",
      "Episode 188250: reward: 24.632233190004822, Avg: 2.65918812130302, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.5080000000000002, mean move frd: 1.5330018147334805\n",
      "\n",
      "Episode 188500: reward: 1.6868709910127269, Avg: 2.9005884191922164, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.08, mean move frd: 0.03693505571323907\n",
      "\n",
      "Episode 188750: reward: 6.386247911744702, Avg: 6.629965301831925, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.34, mean move frd: 0.9039695595064401\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 189000: reward: 397.7302918467624, Avg: 162.78472153929388, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 9.02, mean move frd: 0.016384950337314973\n",
      "\n",
      "Episode 189250: reward: 7.725557562166564, Avg: 14.486483693030403, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.484, mean move frd: -0.04287763977559846\n",
      "\n",
      "Episode 189500: reward: -65.34001989837726, Avg: 4.508357340062315, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.62, mean move frd: 1.327130943471594\n",
      "\n",
      "Episode 189750: reward: 12.03999791521281, Avg: 17.53477828157467, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.836, mean move frd: -0.08431105435381862\n",
      "\n",
      "Episode 190000: reward: 8.475179363054876, Avg: 30.49485980123408, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.544, mean move frd: -0.2681494062293618\n",
      "\n",
      "Episode 190250: reward: -8.528175436625961, Avg: 21.666653485991453, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.164, mean move frd: -0.6656476678732897\n",
      "\n",
      "Episode 190500: reward: 25.86476743071215, Avg: 35.24582390261479, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.46, mean move frd: 0.12735028550768407\n",
      "\n",
      "Episode 190750: reward: 16.158228977762107, Avg: 12.727184433055069, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.516, mean move frd: 0.2541362914045286\n",
      "\n",
      "Episode 191000: reward: 1.944169346647847, Avg: 127.03508718934859, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 7.4, mean move frd: -3.2791833300050603\n",
      "\n",
      "Episode 191250: reward: 28.389088830545777, Avg: 48.17725808269566, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.16, mean move frd: -0.355861889574125\n",
      "\n",
      "Episode 191500: reward: -4.277511533053269, Avg: 25.271169127104088, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.168, mean move frd: 0.039448762084133726\n",
      "\n",
      "Episode 191750: reward: -11.398200896984749, Avg: 61.93838896919101, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 4.136, mean move frd: -1.1247267774095688\n",
      "\n",
      "Episode 192000: reward: 88.70772577052512, Avg: 24.843214494954143, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.716, mean move frd: -0.23446166989114983\n",
      "\n",
      "Episode 192250: reward: 29.2250148512887, Avg: 70.50647761916706, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 4.144, mean move frd: 0.012821635013616061\n",
      "\n",
      "Episode 192500: reward: 40.31772660458621, Avg: 45.71562434324712, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.6839999999999997, mean move frd: 0.6825487951008314\n",
      "\n",
      "Episode 192750: reward: 10.824296762408565, Avg: 22.16260627482574, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.972, mean move frd: -0.12404230198232946\n",
      "\n",
      "Episode 193000: reward: -6.362641285668381, Avg: 22.22544824978693, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.18, mean move frd: -1.2772629498405563\n",
      "\n",
      "Episode 193250: reward: 87.46083709239385, Avg: 67.90417311531444, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 4.628, mean move frd: 0.6148642077280049\n",
      "\n",
      "Episode 193500: reward: 17.317429410222818, Avg: 248.89764920405977, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 12.424000000000001, mean move frd: -0.19377117040241534\n",
      "\n",
      "Episode 193750: reward: -5.118677412566034, Avg: 134.73132476699422, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 7.66, mean move frd: 0.025738049765926262\n",
      "\n",
      "Episode 194000: reward: 411.5972994284071, Avg: 181.75210688667346, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 9.432, mean move frd: -0.05505547275975499\n",
      "\n",
      "Episode 194250: reward: 18.31597497332304, Avg: 21.519949069429078, model improved 1 times in 250 episodes.\n",
      "Mean survival time: 1.864, mean move frd: 0.5068272722598463\n",
      "\n",
      "Episode 194500: reward: 20.229544965892146, Avg: 75.63984365459913, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 4.404, mean move frd: -0.28042862121158985\n",
      "\n",
      "Episode 194750: reward: 184.75649966962288, Avg: 36.55031478130771, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.58, mean move frd: -0.9817770299214086\n",
      "\n",
      "Episode 195000: reward: 338.32647791145627, Avg: 98.63291032787625, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 5.167999999999999, mean move frd: -0.37471104554166285\n",
      "\n",
      "Episode 195250: reward: 26.8803775102313, Avg: 191.92256799620725, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 9.652000000000001, mean move frd: -0.5360152330249035\n",
      "\n",
      "Episode 195500: reward: 44.38773942706071, Avg: 148.7804072063836, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 7.96, mean move frd: -0.1009058812152741\n",
      "\n",
      "Episode 195750: reward: 359.32472596726865, Avg: 56.44287037516936, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.352, mean move frd: 0.31502591816278225\n",
      "\n",
      "Episode 196000: reward: 41.195870731656946, Avg: 172.32717309944726, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 9.040000000000001, mean move frd: 1.343514797902544\n",
      "\n",
      "Episode 196250: reward: 1.2430755684512746, Avg: 87.57232447757534, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 5.132000000000001, mean move frd: -0.686572922821245\n",
      "\n",
      "Episode 196500: reward: 8.286096918563809, Avg: 22.825784765755735, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.9440000000000002, mean move frd: 0.1900995543817115\n",
      "\n",
      "Episode 196750: reward: 97.3237560108306, Avg: 95.50108197221986, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 5.324, mean move frd: 0.32761648043122565\n",
      "\n",
      "Episode 197000: reward: 133.52877403377155, Avg: 51.57623244505389, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.404, mean move frd: -0.5283143175509363\n",
      "\n",
      "Episode 197250: reward: 8.837860884396768, Avg: 14.817165534638212, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.4440000000000002, mean move frd: -0.12248454464016675\n",
      "\n",
      "Episode 197500: reward: 5.317137424169458, Avg: 16.508205370395512, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.6880000000000002, mean move frd: -0.0990763076288057\n",
      "\n",
      "Episode 197750: reward: 25.109236470972714, Avg: 10.705591365770575, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.036, mean move frd: -0.2534643580025338\n",
      "\n",
      "Episode 198000: reward: 3.009278856416296, Avg: 5.0258044734454845, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.216, mean move frd: -0.0591648148219082\n",
      "\n",
      "Episode 198250: reward: -1.7895519863800047, Avg: -6.469671872347304, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 0.6759999999999999, mean move frd: 0.8451147513597096\n",
      "\n",
      "Episode 198500: reward: 418.82354148641633, Avg: 118.69062031928249, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 6.648, mean move frd: -0.7580487694288089\n",
      "\n",
      "Episode 198750: reward: 75.84603696366663, Avg: 17.163938609910467, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.68, mean move frd: -0.017150006244802497\n",
      "\n",
      "Episode 199000: reward: 5.926164272339502, Avg: 11.759897302498642, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.216, mean move frd: 0.24514391668715838\n",
      "\n",
      "Episode 199250: reward: 10.472289407224174, Avg: 63.038509884946244, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.668, mean move frd: -0.18038850433288936\n",
      "\n",
      "Episode 199500: reward: 418.31079865577397, Avg: 193.55561625368873, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 9.648, mean move frd: -0.2024778284998487\n",
      "\n",
      "Episode 199750: reward: 414.98082315333943, Avg: 136.58288334235516, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 7.252000000000001, mean move frd: -0.12677464437999317\n",
      "\n",
      "Episode 200000: reward: 7.6127427710057916, Avg: 16.506433151617905, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.584, mean move frd: 0.045114253850538424\n",
      "\n",
      "Episode 200250: reward: 440.1060610078235, Avg: 89.53413121296425, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 5.012, mean move frd: -1.1503381609034158\n",
      "\n",
      "Episode 200500: reward: -17.92571905835667, Avg: 128.19789436416545, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 6.932, mean move frd: -0.8220180587323718\n",
      "\n",
      "Episode 200750: reward: 16.267409211072803, Avg: 37.03999782514656, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.56, mean move frd: 0.1908346756699498\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 201000: reward: 194.17611251845946, Avg: 197.8646694978798, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 10.196, mean move frd: -0.4262461631177299\n",
      "\n",
      "Episode 201250: reward: 42.25667448016597, Avg: 35.99266674148906, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 2.4, mean move frd: 0.3149907458943109\n",
      "\n",
      "Episode 201500: reward: 4.725094945488575, Avg: 116.91791995369064, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 6.38, mean move frd: -0.42121835762031934\n",
      "\n",
      "Episode 201750: reward: 9.2349251786302, Avg: 186.05129670708448, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 9.376000000000001, mean move frd: 0.4570014022151582\n",
      "\n",
      "Episode 202000: reward: 125.4224593214619, Avg: 63.20353471436435, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.988, mean move frd: 0.323335133114865\n",
      "\n",
      "Episode 202250: reward: -6.980586054295426, Avg: 41.852707810114765, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 3.3200000000000003, mean move frd: -0.0034903795369753076\n",
      "\n",
      "Episode 202500: reward: 108.31571141291369, Avg: 237.00303545268267, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 11.915999999999999, mean move frd: 0.7550120952735392\n",
      "\n",
      "Episode 202750: reward: 16.557866656173985, Avg: 22.51894200768797, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.92, mean move frd: 0.6970129952577722\n",
      "\n",
      "Episode 203000: reward: -5.422498061877883, Avg: 8.10251794513545, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 1.344, mean move frd: -0.02892149078663726\n",
      "\n",
      "Episode 203250: reward: 72.39448140805963, Avg: 70.60024357326522, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 4.408, mean move frd: -0.6005897470437841\n",
      "\n",
      "Episode 203500: reward: 77.85039598485949, Avg: 188.8385741710012, model improved 0 times in 250 episodes.\n",
      "Mean survival time: 10.016, mean move frd: -0.6114191445609404\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script is used as main for learning\n",
    "\"\"\"\n",
    "\n",
    "env = GymWrapper(ConfigurableANYmal())\n",
    "env.env.enable_reward_terminal = False\n",
    " \n",
    "\n",
    "## Observation and action space size\n",
    "observationNum = 19+18 #env.observation_space.shape[0]\n",
    "actionNum = env.action_space.shape[0]\n",
    "totalNum = observationNum + actionNum\n",
    "\n",
    "## Training Parameters\n",
    "epochs = 500000\n",
    "batchSize = 64\n",
    "gpu = False\n",
    "rewardTracking = []\n",
    "rewardAvg = []\n",
    "replayMemorySize = 5000\n",
    "printE = 250\n",
    "path = '/home/linghao/Desktop/'\n",
    "updateStart = 1000\n",
    "policyDelay = 3\n",
    "## Initialise TD3 agent\n",
    "\n",
    "# Some implementation suggest a single output from critic - can try\n",
    "agent = TD3Agent(env.action_space,\n",
    "                (observationNum, 256, 128, actionNum), \n",
    "                (totalNum, 256, 128, actionNum), \n",
    "                actorLRate = 0.00001, criticLRate = 0.00001, criticLoss = 'HL', gamma = 0.95, targetUpdaterLRate = 0.05,\n",
    "                replayMemorySize = replayMemorySize, gpu=gpu, batchSize=batchSize, policyDelay = policyDelay)\n",
    "# agent.loadModel('/home/linghao/Desktop/TD3_anymal_stable/')\n",
    "agent.trainMode()\n",
    "trackR, avgR = train(agent, env, epochs=epochs, replayMemorySize = replayMemorySize, updateStart = updateStart, printEvery=printE, savePath=path)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bb9d7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/linghao/Desktop/td3_200k.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump([trackR, avgR], '/home/linghao/Desktop/td3_200k.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b38c3adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.saveModel(path = path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b619d3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fca5573b850>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trackR)\n",
    "plt.plot(avgR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae2e2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.env._action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a26dcc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym\n",
    "# from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "import gym\n",
    "# from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from gym_jiminy.envs import ANYmalJiminyEnv\n",
    "import numpy as np\n",
    "## Observation and action space size\n",
    "observationNum = 37\n",
    "actionNum = 12\n",
    "totalNum = observationNum + actionNum\n",
    "\n",
    "## Training Parameters\n",
    "epochs = 200\n",
    "batchSize = 128\n",
    "gpu = False\n",
    "rewardTracking = []\n",
    "rewardAvg = []\n",
    "replayMemorySize = 10000\n",
    "printE = 10\n",
    "path = '/home/linghao/Desktop/'\n",
    "updateStart = 5000\n",
    "policyDelay = 4\n",
    "## Initialise TD3 agent\n",
    "\n",
    "newEnv = GymWrapper(ConfigurableANYmal())\n",
    " \n",
    "\n",
    "agentEval = TD3Agent(newEnv.action_space,\n",
    "                (observationNum, 256, 128, actionNum), \n",
    "                (totalNum, 256, 128, actionNum), \n",
    "                actorLRate = 0.0001, criticLRate = 0.0001, criticLoss = 'HL', gamma = 0.95, targetUpdaterLRate = 0.05,\n",
    "                replayMemorySize = replayMemorySize, gpu=gpu, batchSize=batchSize, policyDelay = policyDelay)\n",
    "\n",
    "agentEval.loadModel('/home/linghao/Desktop')\n",
    "agentEval.evalMode()\n",
    "\n",
    "\n",
    "observation = newEnv.reset()\n",
    "\n",
    " \n",
    "# video_recorder = VideoRecorder(newEnv, '/home/linghao/Desktop/Ant.mp4', enabled=True)\n",
    "\n",
    "# R = 0\n",
    "\n",
    "# for Iter in range(7000):\n",
    "# #     video_recorder.capture_frame()\n",
    "#     observation, reward, terminated, truncated, info = newEnv.step(agentEval.getAction(observation, eval = True))\n",
    "# #     observation, reward, terminated, info = newEnv.env.step() \n",
    "#     R += reward\n",
    "    \n",
    "#     if terminated:\n",
    "#         print('Term')\n",
    "#         break\n",
    "# #     elif truncated:\n",
    "# #         print('Outbound')\n",
    "# #         break\n",
    "#     else:\n",
    "#         continue\n",
    "# # video_recorder.close()\n",
    "# # video_recorder.enabled = False\n",
    "\n",
    "# newEnv.stop()\n",
    "# camera_xyzrpy = ([5.0, 2.0, 0.0], [np.pi/2, 0.0, np.pi/1.5])\n",
    "# newEnv.env.simulator.replay(camera_xyzrpy=camera_xyzrpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "96def6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 0\n",
    "\n",
    "from gym_jiminy.envs import ANYmalPDControlJiminyEnv\n",
    "from gym_jiminy.envs import ANYmalJiminyEnv\n",
    "\n",
    "newEnv = GymWrapper(ConfigurableANYmal())\n",
    "newEnv.env.seed()\n",
    "\n",
    "observation = newEnv.reset()\n",
    "ac = []\n",
    "inf = []\n",
    "# agent.evalMode()\n",
    "\n",
    "\n",
    "for Iter in range(7000):\n",
    "#     video_recorder.capture_frame()\n",
    "    actions = agentEval.getAction(observation, eval = False)\n",
    "    observation, reward, terminated, truncated, info = newEnv.step(actions)\n",
    "    ac.append(actions)\n",
    "    inf.append(info)\n",
    "#     observation, reward, terminated, info = newEnv.step(newEnv.action_space.sample()) \n",
    "    R += reward\n",
    "    \n",
    "    if terminated:\n",
    "#         print('Term')\n",
    "#         newEnv.env.reset()\n",
    "        break\n",
    "#     elif truncated:\n",
    "#         print('Outbound')\n",
    "#         break\n",
    "    else:\n",
    "        continue\n",
    "# video_recorder.close()\n",
    "# video_recorder.enabled = False\n",
    "\n",
    "\n",
    "# newEnv.stop()\n",
    "# camera_xyzrpy = ([5.0, 2.0, 1.0], [np.pi/2, 0.0, np.pi/1.5])\n",
    "# newEnv.simulator.replay(camera_xyzrpy=camera_xyzrpy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "764a289c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.68"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Iter *0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a4aec126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157.11438009560862"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bdf8c4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "newEnv.env.observe_dt = 0.04\n",
    "newEnv.stop()\n",
    "camera_xyzrpy = ([5.0, 2.0, 1.0], [np.pi/2, 0.0, np.pi/1.5])\n",
    "newEnv.simulator.replay(camera_xyzrpy=camera_xyzrpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "816b7e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "newEnv.simulator.stop()\n",
    "newEnv.simulator.close()\n",
    "newEnv.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8232652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agentEval.rMemory.isFull()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a911950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "newEnv.env._height_neutral \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8ed9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a039337",
   "metadata": {},
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705dca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "835f1f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "newEnv.env.replay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "734cf7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "newEnv.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd708f09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
